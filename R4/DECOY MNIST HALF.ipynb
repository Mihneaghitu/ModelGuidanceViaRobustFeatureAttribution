{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R4 on DECOY_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training import certified_training_utils as ct_utils\n",
    "from models.fully_connected import FCNAugmented\n",
    "from models.robust_regularizer import input_gradient_interval_regularizer\n",
    "from models.pipeline import (train_model_with_certified_input_grad, train_model_with_pgd_robust_input_grad,\n",
    "                             test_model_accuracy, test_delta_input_robustness, write_results_to_file,\n",
    "                             uniformize_magnitudes_schedule, load_params_or_results_from_file,\n",
    "                             train_model_with_smoothed_input_grad, test_model_avg_and_wg_accuracy)\n",
    "from datasets import decoy_mnist\n",
    "from metrics import get_restart_avg_and_worst_group_accuracy_with_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pre-training\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "SEED = 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "ARCH = (784, 10, 512, 3, False)\n",
    "model = FCNAugmented(*ARCH).to(device)\n",
    "train_batch_size = 1054\n",
    "test_batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_masks_train, dl_masks_test = decoy_mnist.get_half_decoy_masked_dataloaders(train_batchsize=train_batch_size, test_batchsize=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient(model, batch_input, batch_labels, batch_mask, epsilon, elem_idx):\n",
    "    batch_input, batch_labels, batch_mask = batch_input.to(device), batch_labels.to(device), batch_mask.to(device)\n",
    "    grad_bounds = input_gradient_interval_regularizer(\n",
    "        model, batch_input, batch_labels, \"cross_entropy\", epsilon, 0.0, return_grads=True, regularizer_type=\"r4\", batch_masks=batch_mask\n",
    "    )\n",
    "    dx_l, dx_u = grad_bounds[1]\n",
    "    dx_n, _ = grad_bounds[0]\n",
    "    fig, ax = plt.subplots(3, 2, figsize=(14, 13))\n",
    "    ax[0][0].imshow(batch_input[elem_idx].cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "    ax[0][0].set_title(f\"Input at index {elem_idx}\")\n",
    "    im_mask = ax[0][1].imshow(batch_mask[elem_idx].cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "    ax[0][1].set_title(f\"Mask at index {elem_idx}\")\n",
    "    fig.colorbar(im_mask, ax=ax[0][1])\n",
    "    im_dx_l = ax[1][0].imshow(dx_l[elem_idx].cpu().detach().numpy().reshape(28, 28), cmap='coolwarm')\n",
    "    ax[1][0].set_title(f\"Lower bound of gradient at index {elem_idx}\")\n",
    "    fig.colorbar(im_dx_l, ax=ax[1][0])\n",
    "    im_dx_u = ax[1][1].imshow(dx_u[elem_idx].cpu().detach().numpy().reshape(28, 28), cmap='coolwarm')\n",
    "    ax[1][1].set_title(f\"Upper bound of gradient at index {elem_idx}\")\n",
    "    fig.colorbar(im_dx_u, ax=ax[1][1])\n",
    "    im_dx_n = ax[2][0].imshow(dx_n[elem_idx].cpu().detach().numpy().reshape(28, 28), cmap='coolwarm')\n",
    "    ax[2][0].set_title(f\"Gradient at index {elem_idx}\")\n",
    "    fig.colorbar(im_dx_n, ax=ax[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_idx = torch.randint(0, train_batch_size, (1,)).item()\n",
    "print(elem_idx)\n",
    "train_batch = list(next(iter(dl_masks_train)))\n",
    "print(train_batch[0][elem_idx][0][0])\n",
    "print(train_batch[0][elem_idx][27][27])\n",
    "visualize_gradient(model, train_batch[0], train_batch[1], train_batch[2], 0.1, elem_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_INPUT_ROBUSTNESS_PARAM = 0.5\n",
    "model_root_save_dir = \"saved_experiment_models/performance/half_decoy_mnist/\"\n",
    "os.makedirs(model_root_save_dir, exist_ok=True)\n",
    "# r4_pmo is the same as r4 but with only the masked region perturbed\n",
    "methods = [\"std\", \"r3\", \"r4\", \"ibp_ex\", \"ibp_ex+r3\", \"pgd_r4\", \"rand_r4\", \"smooth_r3\"]\n",
    "save_dir_for_method = {method: os.path.join(model_root_save_dir, method) for method in methods}\n",
    "for method in methods:\n",
    "    os.makedirs(save_dir_for_method[method], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_method = \"std\"\n",
    "# hyperparameters\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k, weight_decay = 40, 0.0001, 3, 0.1, -1, -1, 1e-4\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "train_acc, test_acc, num_robust, avg_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 0, 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = FCNAugmented(*ARCH)\n",
    "\n",
    "    print(f\"========== Training model with method {std_method} restart {i} ==========\")\n",
    "    train_model_with_certified_input_grad(dl_masks_train, num_epochs, curr_model, lr, criterion, epsilon, std_method, k, device, False, weight_decay=weight_decay)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_masks_train, device, multi_class=True)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_masks_test, device, multi_class=True)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_masks_test, curr_model, epsilon, DELTA_INPUT_ROBUSTNESS_PARAM, \"cross_entropy\", device)\n",
    "    num_robust += num_robust\n",
    "    avg_delta += min_delta\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    avg_g_acc, wg_acc, wg = test_model_avg_and_wg_accuracy(curr_model, dl_masks_test, device, num_groups=10, multi_class=True)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[std_method], f\"run_{i}.pt\"))\n",
    "empty_model = FCNAugmented(*ARCH)\n",
    "avg_acc, wg_acc, wg, *_ = get_restart_avg_and_worst_group_accuracy_with_stddev(\n",
    "    dl_masks_test, save_dir_for_method[std_method], empty_model, device, num_groups=10, multi_class=True\n",
    ")\n",
    "write_results_to_file(\"experiment_results/half_decoy_mnist.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_robust_delta\": round(avg_delta / restarts, 5),\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 5),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 5)\n",
    "                       }, std_method)\n",
    "write_results_to_file(\"experiment_results/half_decoy_mnist_params.yaml\",\n",
    "                        {\"epsilon\": epsilon,\n",
    "                         \"test_epsilon\": epsilon,\n",
    "                         \"k\": k,\n",
    "                         \"weight_coeff\": weight_coeff,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": True,\n",
    "                         \"has_conv\": False,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"with_k_schedule\": False}, std_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RRR Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrr_method = \"r3\"\n",
    "# rebatch\n",
    "train_batch_size = 256\n",
    "dl_masks_train = torch.utils.data.DataLoader(dl_masks_train.dataset, batch_size=train_batch_size, shuffle=True)\n",
    "# hyperparams\n",
    "num_epochs, lr, restarts, epsilon, k, weight_decay = 25, 0.0001, 4, 0.1, 5, 3e-4\n",
    "train_acc, test_acc, num_robust, avg_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 0, 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = FCNAugmented(*ARCH)\n",
    "\n",
    "    print(f\"========== Training model with method {rrr_method}, restart {i} ==========\")\n",
    "    train_model_with_certified_input_grad(dl_masks_train, num_epochs, curr_model, lr, criterion, epsilon, rrr_method,\n",
    "        k, device, False, weight_decay=weight_decay)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_masks_train, device, multi_class=True)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_masks_test, device, multi_class=True)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_masks_test, curr_model, epsilon, DELTA_INPUT_ROBUSTNESS_PARAM, \"cross_entropy\", device)\n",
    "    num_robust += num_robust\n",
    "    avg_delta += min_delta\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    avg_g_acc, wg_acc, wg = test_model_avg_and_wg_accuracy(curr_model, dl_masks_test, device, num_groups=10, multi_class=True)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[rrr_method], f\"run_{i}.pt\"))\n",
    "empty_model = FCNAugmented(*ARCH)\n",
    "avg_acc, wg_acc, wg, *_ = get_restart_avg_and_worst_group_accuracy_with_stddev(\n",
    "    dl_masks_test, save_dir_for_method[rrr_method], empty_model, device, num_groups=10, multi_class=True\n",
    ")\n",
    "write_results_to_file(\"experiment_results/half_decoy_mnist.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_robust_delta\": round(avg_delta / restarts, 5),\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 5),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 5)\n",
    "                       }, rrr_method)\n",
    "write_results_to_file(\"experiment_results/half_decoy_mnist_params.yaml\",\n",
    "                        {\"epsilon\": epsilon,\n",
    "                         \"test_epsilon\": epsilon,\n",
    "                         \"k\": k,\n",
    "                         \"weight_coeff\": weight_coeff,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": True,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"has_conv\": False,\n",
    "                         \"with_k_schedule\": False}, rrr_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "r4_method = \"r4\"\n",
    "num_epochs, lr, restarts, epsilon, k, weight_decay = 20, 0.001, 3, 0.4, 0.1, 1e-5\n",
    "test_epsilon = 0.1\n",
    "train_acc, test_acc, num_robust, avg_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 0, 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + 65)\n",
    "    curr_model = FCNAugmented(*ARCH)\n",
    "\n",
    "    print(f\"========== Training model with method R4, restart {i} ==========\")\n",
    "    train_model_with_certified_input_grad(dl_masks_train, num_epochs, curr_model, lr, criterion, epsilon, r4_method, k,\n",
    "        device, False, weight_decay=weight_decay)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_masks_train, device, multi_class=True)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_masks_test, device, multi_class=True)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_masks_test, curr_model, test_epsilon, DELTA_INPUT_ROBUSTNESS_PARAM, \"cross_entropy\", device)\n",
    "    num_robust += n_r\n",
    "    avg_delta += min_delta\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    avg_g_acc, wg_acc, wg = test_model_avg_and_wg_accuracy(curr_model, dl_masks_test, device, num_groups=10, multi_class=True)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[r4_method], f\"run_{i}.pt\"))\n",
    "empty_model = FCNAugmented(*ARCH)\n",
    "avg_acc, wg_acc, wg, *_ = get_restart_avg_and_worst_group_accuracy_with_stddev(\n",
    "    dl_masks_test, save_dir_for_method[r4_method], empty_model, device, num_groups=10, multi_class=True\n",
    ")\n",
    "# write_results_to_file(\"experiment_results/half_decoy_mnist.yaml\",\n",
    "#                       {\"train_acc\": round(train_acc / restarts, 5),\n",
    "#                        \"test_acc\": round(test_acc / restarts, 5),\n",
    "#                        \"avg_group_acc\": round(avg_acc, 5),\n",
    "#                        \"worst_group_acc\": round(wg_acc, 5),\n",
    "#                        \"worst_group\": wg,\n",
    "#                        \"min_robust_delta\": round(avg_delta / restarts, 5),\n",
    "#                        \"min_lower_bound\": round(min_lower_bound / restarts, 5),\n",
    "#                        \"max_upper_bound\": round(max_upper_bound / restarts, 5)\n",
    "#                        }, r4_method)\n",
    "# write_results_to_file(\"experiment_results/half_decoy_mnist_params.yaml\",\n",
    "#                         {\"epsilon\": epsilon,\n",
    "#                          \"test_epsilon\": test_epsilon,\n",
    "#                          \"k\": k,\n",
    "#                          \"weight_decay\": weight_decay,\n",
    "#                          \"num_epochs\": num_epochs,\n",
    "#                          \"lr\": lr,\n",
    "#                          \"restarts\": restarts,\n",
    "#                          \"train_batch_size\": dl_masks_train.batch_size,\n",
    "#                          \"test_batch_size\": dl_masks_test.batch_size,\n",
    "#                          \"class_weights\": -1,\n",
    "#                          \"multi_class\": True,\n",
    "#                          \"has_conv\": False,\n",
    "#                          \"weight_decay\": weight_decay,\n",
    "#                          \"with_k_schedule\": False}, r4_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBP-EX Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibp_ex_method = \"ibp_ex\"\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k = 40, 0.001, 3, 0., 0.0002, 4\n",
    "test_epsilon = 0.1\n",
    "train_acc, test_acc, num_robust, avg_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 0, 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = FCNAugmented(*ARCH)\n",
    "\n",
    "    print(f\"========== Training model with method {ibp_ex_method} restart {i} ==========\")\n",
    "    train_model_with_certified_input_grad(dl_masks_train, num_epochs, curr_model, lr, criterion, epsilon, ibp_ex_method, k,\n",
    "                                          device, False, weight_reg_coeff=weight_coeff)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_masks_train, device, multi_class=True)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_masks_test, device, multi_class=True)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_masks_test, curr_model, test_epsilon, DELTA_INPUT_ROBUSTNESS_PARAM, \"cross_entropy\", device)\n",
    "    num_robust += n_r\n",
    "    avg_delta += min_delta\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    avg_g_acc, wg_acc, wg = test_model_avg_and_wg_accuracy(curr_model, dl_masks_test, device, num_groups=10, multi_class=True)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[ibp_ex_method], f\"run_{i}.pt\"))\n",
    "empty_model = FCNAugmented(*ARCH)\n",
    "avg_acc, wg_acc, wg, *_ = get_restart_avg_and_worst_group_accuracy_with_stddev(\n",
    "    dl_masks_test, save_dir_for_method[ibp_ex_method], empty_model, device, num_groups=10, multi_class=True\n",
    ")\n",
    "write_results_to_file(\"experiment_results/decoy_mnist.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_robust_delta\": round(avg_delta / restarts, 5),\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 5),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 5)\n",
    "                       }, ibp_ex_method)\n",
    "write_results_to_file(\"experiment_results/decoy_mnist_params.yaml\",\n",
    "                        {\"epsilon\": epsilon,\n",
    "                         \"test_epsilon\": test_epsilon,\n",
    "                         \"k\": k,\n",
    "                         \"weight_coeff\": weight_coeff,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": True,\n",
    "                         \"has_conv\": False,\n",
    "                         \"with_k_schedule\": False}, ibp_ex_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBP-EX + R3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Training model with method ibp_ex+r3, restart 0 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  22%|██▎       | 9/40 [00:47<02:42,  5.23s/it, loss (entropy)=2.31, reg=tensor(0.7903, device='cuda:0', grad_fn=<MulBackward0>)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m curr_model \u001b[38;5;241m=\u001b[39m FCNAugmented(\u001b[38;5;241m*\u001b[39mARCH)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m========== Training model with method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mibp_ex_and_r3_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, restart \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ==========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain_model_with_certified_input_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_masks_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mibp_ex_and_r3_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting model accuracy for the training set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m test_model_accuracy(curr_model, dl_masks_train, device, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/R4/examples/models/pipeline.py:44\u001b[0m, in \u001b[0;36mtrain_model_with_certified_input_grad\u001b[0;34m(dl_train, n_epochs, model, learning_rate, criterion, epsilon, mlx_method, k, device, has_conv, k_schedule, weight_reg_coeff, class_weights, weight_decay, suppress_tqdm)\u001b[0m\n\u001b[1;32m     41\u001b[0m u, x, m \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mto(device), x\u001b[38;5;241m.\u001b[39mto(device), m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# For std, we will waste some time doing the bounds, but at least it is consistent across methods\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m inp_grad_reg \u001b[38;5;241m=\u001b[39m \u001b[43minput_gradient_interval_regularizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularizer_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlx_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_conv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_conv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_reg_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_reg_coeff\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mlx_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m inp_grad_reg \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/R4/examples/models/robust_regularizer.py:68\u001b[0m, in \u001b[0;36minput_gradient_interval_regularizer\u001b[0;34m(model, batch, labels, loss_fn, epsilon, model_epsilon, return_grads, regularizer_type, batch_masks, has_conv, device, weight_reg_coeff)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# remove the first module (copy of model) and the last module (sigmoid or softmax)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# if the first module is a DataParallel, remove the first two instead\u001b[39;00m\n\u001b[1;32m     67\u001b[0m modules \u001b[38;5;241m=\u001b[39m modules[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules[\u001b[38;5;241m0\u001b[39m], torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel) \u001b[38;5;28;01melse\u001b[39;00m modules[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 68\u001b[0m params_n_dense \u001b[38;5;241m=\u001b[39m \u001b[43mct_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# do a forward pass without gradients to be able to call the loss gradient bounding functions in the agt module\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/R4/abstract_gradient_training/certified_training_utils.py:128\u001b[0m, in \u001b[0;36mget_parameters\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    126\u001b[0m param_n \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m param_n \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]  \u001b[38;5;66;03m# flatten the list\u001b[39;00m\n\u001b[1;32m    127\u001b[0m param_n \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m param_n]  \u001b[38;5;66;03m# reshape bias to [n x 1] instead of [n]\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m param_n \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m param_n]\n\u001b[1;32m    129\u001b[0m param_l \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m param_n]\n\u001b[1;32m    130\u001b[0m param_u \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m param_n]\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/R4/abstract_gradient_training/certified_training_utils.py:128\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m param_n \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m param_n \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]  \u001b[38;5;66;03m# flatten the list\u001b[39;00m\n\u001b[1;32m    127\u001b[0m param_n \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m param_n]  \u001b[38;5;66;03m# reshape bias to [n x 1] instead of [n]\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m param_n \u001b[38;5;241m=\u001b[39m [\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m param_n]\n\u001b[1;32m    129\u001b[0m param_l \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m param_n]\n\u001b[1;32m    130\u001b[0m param_u \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m param_n]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ibp_ex_and_r3_method = \"ibp_ex+r3\"\n",
    "num_epochs, lr, restarts, epsilon, k, weight_decay = 40, 0.002, 3, 0.3, 0.2, 1e-8\n",
    "test_epsilon = 0.1\n",
    "train_acc, test_acc, num_robust, avg_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 0, 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i)\n",
    "    curr_model = FCNAugmented(*ARCH)\n",
    "\n",
    "    print(f\"========== Training model with method {ibp_ex_and_r3_method}, restart {i} ==========\")\n",
    "    train_model_with_certified_input_grad(dl_masks_train, num_epochs, curr_model, lr, criterion, epsilon, ibp_ex_and_r3_method, k,\n",
    "        device, False, weight_decay=weight_decay)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_masks_train, device, multi_class=True)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_masks_test, device, multi_class=True)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_masks_test, curr_model, test_epsilon, DELTA_INPUT_ROBUSTNESS_PARAM, \"cross_entropy\", device)\n",
    "    num_robust += n_r\n",
    "    avg_delta += min_delta\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    avg_g_acc, wg_acc, wg = test_model_avg_and_wg_accuracy(curr_model, dl_masks_test, device, num_groups=10, multi_class=True)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[ibp_ex_and_r3_method], f\"run_{i}.pt\"))\n",
    "empty_model = FCNAugmented(*ARCH)\n",
    "avg_acc, wg_acc, wg, *_ = get_restart_avg_and_worst_group_accuracy_with_stddev(\n",
    "    dl_masks_test, save_dir_for_method[ibp_ex_and_r3_method], empty_model, device, num_groups=10, multi_class=True\n",
    ")\n",
    "# write_results_to_file(\"experiment_results/half_decoy_mnist.yaml\",\n",
    "#                       {\"train_acc\": round(train_acc / restarts, 5),\n",
    "#                        \"test_acc\": round(test_acc / restarts, 5),\n",
    "#                        \"avg_group_acc\": round(avg_acc, 5),\n",
    "#                        \"worst_group_acc\": round(wg_acc, 5),\n",
    "#                        \"worst_group\": wg,\n",
    "#                        \"min_robust_delta\": round(avg_delta / restarts, 5),\n",
    "#                        \"min_lower_bound\": round(min_lower_bound / restarts, 5),\n",
    "#                        \"max_upper_bound\": round(max_upper_bound / restarts, 5)\n",
    "#                        }, ibp_ex_and_r3_method)\n",
    "# write_results_to_file(\"experiment_results/half_decoy_mnist_params.yaml\",\n",
    "#                         {\"epsilon\": epsilon,\n",
    "#                          \"test_epsilon\": test_epsilon,\n",
    "#                          \"k\": k,\n",
    "#                          \"weight_decay\": weight_decay,\n",
    "#                          \"num_epochs\": num_epochs,\n",
    "#                          \"lr\": lr,\n",
    "#                          \"restarts\": restarts,\n",
    "#                          \"train_batch_size\": dl_masks_train.batch_size,\n",
    "#                          \"test_batch_size\": dl_masks_test.batch_size,\n",
    "#                          \"class_weights\": -1,\n",
    "#                          \"multi_class\": True,\n",
    "#                          \"has_conv\": False,\n",
    "#                          \"weight_decay\": weight_decay,\n",
    "#                          \"with_k_schedule\": False}, ibp_ex_and_r3_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGD-R4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_r4 = \"pgd_r4\"\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k = 30, 0.001, 4, 0.5, 0.0001, 14\n",
    "test_epsilon = 0.1\n",
    "train_acc, test_acc, num_robust, avg_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 0, 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = FCNAugmented(*ARCH)\n",
    "\n",
    "    print(f\"========== Training model with method {pgd_r4} restart {i} ==========\")\n",
    "    train_model_with_pgd_robust_input_grad(\n",
    "        dl_masks_train, num_epochs, curr_model, lr, criterion, epsilon, pgd_r4, k, device, weight_reg_coeff=weight_coeff\n",
    "    )\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_masks_train, device, multi_class=True)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_masks_test, device, multi_class=True)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_masks_test, curr_model, test_epsilon, DELTA_INPUT_ROBUSTNESS_PARAM, \"cross_entropy\", device)\n",
    "    num_robust += n_r\n",
    "    avg_delta += min_delta\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    avg_g_acc, wg_acc, wg = test_model_avg_and_wg_accuracy(curr_model, dl_masks_test, device, num_groups=10, multi_class=True)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[pgd_r4], f\"run_{i}.pt\"))\n",
    "empty_model = FCNAugmented(*ARCH)\n",
    "avg_acc, wg_acc, wg, *_ = get_restart_avg_and_worst_group_accuracy_with_stddev(\n",
    "    dl_masks_test, save_dir_for_method[pgd_r4], empty_model, device, num_groups=10, multi_class=True\n",
    ")\n",
    "write_results_to_file(\"experiment_results/decoy_mnist.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_robust_delta\": round(avg_delta / restarts, 5),\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 5),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 5)\n",
    "                       }, pgd_r4)\n",
    "write_results_to_file(\"experiment_results/decoy_mnist_params.yaml\",\n",
    "                        {\"epsilon\": epsilon,\n",
    "                         \"test_epsilon\": test_epsilon,\n",
    "                         \"k\": k,\n",
    "                         \"weight_coeff\": weight_coeff,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": True,\n",
    "                         \"has_conv\": False,\n",
    "                         \"with_k_schedule\": False}, pgd_r4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothed-R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_r3 = \"smooth_r3\"\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k = 25, 0.001, 4, 0.1, 0.0001, 10\n",
    "test_epsilon = 0.1\n",
    "train_acc, test_acc, num_robust, avg_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 0, 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = FCNAugmented(*ARCH)\n",
    "\n",
    "    print(f\"========== Training model with method {smooth_r3} restart {i} ==========\")\n",
    "    train_model_with_smoothed_input_grad(\n",
    "        dl_masks_train, num_epochs, curr_model, lr, criterion, epsilon, smooth_r3, k, device, weight_reg_coeff=weight_coeff\n",
    "    )\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_masks_train, device, multi_class=True)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_masks_test, device, multi_class=True)\n",
    "    _, min_delta, m_l, m_u = test_delta_input_robustness(dl_masks_test, curr_model, test_epsilon, DELTA_INPUT_ROBUSTNESS_PARAM, \"cross_entropy\", device)\n",
    "    avg_delta += min_delta\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    avg_g_acc, wg_acc, wg = test_model_avg_and_wg_accuracy(curr_model, dl_masks_test, device, num_groups=10, multi_class=True)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[smooth_r3], f\"run_{i}.pt\"))\n",
    "empty_model = FCNAugmented(*ARCH)\n",
    "avg_acc, wg_acc, wg, *_ = get_restart_avg_and_worst_group_accuracy_with_stddev(\n",
    "    dl_masks_test, save_dir_for_method[smooth_r3], empty_model, device, num_groups=10, multi_class=True\n",
    ")\n",
    "write_results_to_file(\"experiment_results/decoy_mnist.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_robust_delta\": round(avg_delta / restarts, 5),\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 5),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 5)\n",
    "                       }, smooth_r3)\n",
    "write_results_to_file(\"experiment_results/decoy_mnist_params.yaml\",\n",
    "                        {\"epsilon\": epsilon,\n",
    "                         \"test_epsilon\": test_epsilon,\n",
    "                         \"k\": k,\n",
    "                         \"weight_coeff\": weight_coeff,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": True,\n",
    "                         \"has_conv\": False,\n",
    "                         \"with_k_schedule\": False}, smooth_r3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rand-R4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_r4 = \"rand_r4\"\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k = 30, 0.001, 4, 0.5, 0.0001, 15\n",
    "test_epsilon = 0.1\n",
    "train_acc, test_acc, num_robust, avg_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 0, 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = FCNAugmented(*ARCH)\n",
    "\n",
    "    print(f\"========== Training model with method {rand_r4} restart {i} ==========\")\n",
    "    train_model_with_smoothed_input_grad(\n",
    "        dl_masks_train, num_epochs, curr_model, lr, criterion, epsilon, rand_r4, k, device, weight_reg_coeff=weight_coeff\n",
    "    )\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_masks_train, device, multi_class=True)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_masks_test, device, multi_class=True)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_masks_test, curr_model, test_epsilon, DELTA_INPUT_ROBUSTNESS_PARAM, \"cross_entropy\", device)\n",
    "    num_robust += n_r\n",
    "    avg_delta += min_delta\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    avg_g_acc, wg_acc, wg = test_model_avg_and_wg_accuracy(curr_model, dl_masks_test, device, num_groups=10, multi_class=True)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[rand_r4], f\"run_{i}.pt\"))\n",
    "empty_model = FCNAugmented(*ARCH)\n",
    "avg_acc, wg_acc, wg, *_ = get_restart_avg_and_worst_group_accuracy_with_stddev(\n",
    "    dl_masks_test, save_dir_for_method[rand_r4], empty_model, device, num_groups=10, multi_class=True\n",
    ")\n",
    "write_results_to_file(\"experiment_results/decoy_mnist.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_robust_delta\": round(avg_delta / restarts, 5),\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 5),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 5)\n",
    "                       }, rand_r4)\n",
    "write_results_to_file(\"experiment_results/decoy_mnist_params.yaml\",\n",
    "                        {\"epsilon\": epsilon,\n",
    "                         \"test_epsilon\": test_epsilon,\n",
    "                         \"k\": k,\n",
    "                         \"weight_coeff\": weight_coeff,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": True,\n",
    "                         \"has_conv\": False,\n",
    "                         \"with_k_schedule\": False}, rand_r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
