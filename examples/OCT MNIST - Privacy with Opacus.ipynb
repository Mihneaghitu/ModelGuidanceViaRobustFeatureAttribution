{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certified Finetuning of a Classifier on the OCT-MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "import torch\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training import certified_training_utils as ct_utils\n",
    "from models.deepmind import DeepMindSmall \n",
    "from datasets import oct_mnist\n",
    "import opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opacus doesn't respect my logging handler :(\n",
    "logger = logging.getLogger(\"abstract_gradient_training\")\n",
    "logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model on the private Drusen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(eps, delta)=(3.914, 1.0e-04)\n"
     ]
    }
   ],
   "source": [
    "# configure a training set-up using Opacus to mimic the AGT training loop\n",
    "import opacus.accountants\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = \"cuda:0\"\n",
    "batchsize = 5000\n",
    "lr_decay = 0.3\n",
    "lr_min = 0.001\n",
    "dp_sgd_sigma = 0.962\n",
    "clipping = 1.0\n",
    "learning_rate = 0.3\n",
    "n_epochs = 1\n",
    "\n",
    "model = DeepMindSmall(1, 1)\n",
    "criterion = torch.nn.BCELoss()\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n",
    "\n",
    "dl_train, _ = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1], balanced=True)\n",
    "_, dl_test_drusen = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1, 3])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "privacy_engine = opacus.PrivacyEngine(accountant=\"rdp\")\n",
    "model_private, optimizer_private, data_loader_private = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=dl_train,\n",
    "    noise_multiplier=dp_sgd_sigma,\n",
    "    max_grad_norm=clipping,\n",
    "    poisson_sampling=False,\n",
    ")\n",
    "\n",
    "def get_lr(epoch):\n",
    "    lr = max(1 / (1 + lr_decay * epoch), lr_min / learning_rate)\n",
    "    return lr\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer_private, get_lr)\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    for i, (x, u) in enumerate(dl_train):\n",
    "        # AGT only takes full batches\n",
    "        if u.size(0) < batchsize:\n",
    "            break\n",
    "        u, x = u.to(device), x.to(device)\n",
    "        output = model_private(x)\n",
    "        loss = criterion(output.squeeze().float(), u.squeeze().float())\n",
    "        # Backward and optimize\n",
    "        optimizer_private.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_private.step()\n",
    "        scheduler.step()\n",
    "\n",
    "# compute privacy guarantees\n",
    "delta = 0.0001\n",
    "epsilon = privacy_engine.accountant.get_epsilon(delta=delta)\n",
    "print(f\"(eps, delta)=({epsilon:.4g}, {delta:.1e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/20/2024 18:02:04:INFO:=================== Starting Privacy Certified Training ===================\n",
      "09/20/2024 18:02:04:DEBUG:\tPrivacy parameters: k_private=10, clip_gamma=1.0, dp_sgd_sigma=1.0\n",
      "09/20/2024 18:02:04:DEBUG:\tBounding methods: forward=interval, backward=interval\n",
      "09/20/2024 18:02:04:DEBUG:\tUsing Gaussian privacy-preserving noise (std 1)\n",
      "09/20/2024 18:02:04:INFO:Starting epoch 1\n",
      "09/20/2024 18:02:04:DEBUG:Initialising dataloader batchsize to 5000\n",
      "09/20/2024 18:02:04:INFO:Training batch 1: Network eval bounds=(0.82, 0.82, 0.82), W0 Bound=0.0 \n",
      "09/20/2024 18:02:05:INFO:Training batch 2: Network eval bounds=(0.96, 0.97, 0.98), W0 Bound=0.71 \n",
      "09/20/2024 18:02:06:INFO:Training batch 3: Network eval bounds=(0.57, 0.68, 0.8 ), W0 Bound=1.31 \n",
      "09/20/2024 18:02:07:DEBUG:Skipping batch 4 in epoch 1 (expected batchsize 5000, got 508)\n",
      "09/20/2024 18:02:07:INFO:Final network eval: Network eval bounds=(0.62, 0.87, 0.99), W0 Bound=1.89 \n",
      "09/20/2024 18:02:07:INFO:=================== Finished Privacy Certified Training ===================\n"
     ]
    }
   ],
   "source": [
    "# set up fine-tuning parameters\n",
    "config = AGTConfig(\n",
    "    fragsize=1000,\n",
    "    learning_rate=learning_rate,\n",
    "    n_epochs=n_epochs,\n",
    "    k_private=10,\n",
    "    clip_gamma=clipping,\n",
    "    clip_method=\"clamp\",\n",
    "    dp_sgd_sigma=dp_sgd_sigma,\n",
    "    forward_bound=\"interval\",\n",
    "    device=device,\n",
    "    backward_bound=\"interval\",\n",
    "    loss=\"binary_cross_entropy\",\n",
    "    log_level=\"DEBUG\",\n",
    "    lr_decay=lr_decay,\n",
    "    lr_min=lr_min,\n",
    "    early_stopping=False,\n",
    "    noise_type=\"gaussian\",\n",
    ")\n",
    "\n",
    "# fine-tune the model using abstract gradient training (keeping the convolutional layers fixed)\n",
    "param_l, param_n, param_u = agt.privacy_certified_training(\n",
    "    model, config, dl_train, dl_test_drusen, transform=ct_utils.propagate_conv_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Fine-tuned model accuracy + bounds ===========\n",
      "Class 2 (Drusen) : nominal = 0.87, certified bound = 0.62\n",
      "Classes 0, 1, 3  : nominal = 0.79, certified bound = 0.65\n",
      "All Classes      : nominal = 0.81, certified bound = 0.64\n"
     ]
    }
   ],
   "source": [
    "# get dataloaders, train dataloader is a mix of drusen and the \"healthy\" class\n",
    "dl_train, _ = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1], balanced=True)\n",
    "_, dl_test_drusen = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1, 3])\n",
    "_, dl_test_other = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[2])\n",
    "_, dl_test_all = oct_mnist.get_dataloaders(batchsize, 1000)\n",
    "\n",
    "# evaluate the fine-tuned model\n",
    "drusen_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_drusen)), model, ct_utils.propagate_conv_layers)\n",
    "other_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_other)), model, ct_utils.propagate_conv_layers)\n",
    "all_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers)\n",
    "\n",
    "print(\"=========== Fine-tuned model accuracy + bounds ===========\")\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}, certified bound = {drusen_acc[0]:.2g}\")\n",
    "print(f\"Classes 0, 1, 3  : nominal = {other_acc[1]:.2g}, certified bound = {other_acc[0]:.2g}\")\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}, certified bound = {all_acc[0]:.2g}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
