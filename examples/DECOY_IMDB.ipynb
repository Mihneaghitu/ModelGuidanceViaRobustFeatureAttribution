{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/vol/bitbucket/mg2720/llm/huggingface'\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import abstract_gradient_training as agt\n",
    "import models.llm as llm\n",
    "import models.robust_regularizer\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training import certified_training_utils as ct_utils\n",
    "from models.pipeline import train_llm_with_guidance, test_llm_accuracy, write_results_to_file\n",
    "from datasets.imdb import get_loader_from_dataset, ImdbDataset\n",
    "from datasets.spurious_words import all_imdb_spur\n",
    "from metrics import llm_restart_avg_and_worst_group_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "test_batch_size = 250\n",
    "criterion = torch.nn.BCELoss()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_tokenizer = llm.BertTokenizerWrapper(all_imdb_spur())\n",
    "decoy_kwargs = {\"pos_decoy_word\": \"spielberg\", \"neg_decoy_word\": \"alas\"}\n",
    "imdb_train = ImdbDataset(is_train=True, decoy_kwargs=decoy_kwargs)\n",
    "imdb_test = ImdbDataset(is_train=False, decoy_kwargs=decoy_kwargs, grouped=True)\n",
    "dl_masks_train, dl_masks_test = get_loader_from_dataset(imdb_train, batch_size=batch_size), get_loader_from_dataset(imdb_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root_save_dir = \"saved_experiment_models/performance/imdb_decoy\"\n",
    "os.makedirs(model_root_save_dir, exist_ok=True)\n",
    "methods = [\"std\", \"r3\", \"smooth_r3\", \"rand_r4\", \"pgd_r4\"]\n",
    "save_dir_for_method = {method: os.path.join(model_root_save_dir, method) for method in methods}\n",
    "for method in methods:\n",
    "    os.makedirs(save_dir_for_method[method], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Training model with method std restart 0 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "Epoch: 100%|██████████| 2/2 [20:51<00:00, 625.63s/it, loss=0.000119, reg=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model accuracy for the training set\n",
      "Accuracy = 1\n",
      "Testing model accuracy for the test set\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ImdbDataset' object has no attribute '_ImdbDataset__make_test_decoy_review'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting model accuracy for the test set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     test_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtest_llm_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_masks_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(curr_model\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir_for_method[std_method], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     19\u001b[0m empty_model \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mBertModelWrapper(\u001b[38;5;241m1\u001b[39m, device)\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/fypvenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/R4/examples/models/pipeline.py:276\u001b[0m, in \u001b[0;36mtest_llm_accuracy\u001b[0;34m(model, tokenizer, dl_test, device, multi_class, suppress_log)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_llm_accuracy\u001b[39m(\n\u001b[1;32m    268\u001b[0m     model: LLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m     suppress_log: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    275\u001b[0m     n_corr, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text, labels, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;129;01min\u001b[39;00m dl_test:\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    279\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/fypvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/fypvenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/fypvenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/fypvenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/R4/examples/datasets/imdb.py:68\u001b[0m, in \u001b[0;36mImdbDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m             review \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneg_decoy_word \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m review\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m         review \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__make_test_decoy_review\u001b[49m(review)\n\u001b[1;32m     70\u001b[0m spur_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspurious_words[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb_bad_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspurious_words[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb_bad_neg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     71\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImdbDataset' object has no attribute '_ImdbDataset__make_test_decoy_review'"
     ]
    }
   ],
   "source": [
    "std_method = \"std\"\n",
    "# hyperparameters\n",
    "num_epochs, lr, restarts, weight_decay, lmbda = 2, 0.0001, 1, 0, 0\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "train_acc, test_acc = 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + seed)\n",
    "    curr_model = llm.BertModelWrapper(1, device)\n",
    "\n",
    "    print(f\"========== Training model with method {std_method} restart {i} ==========\")\n",
    "    train_llm_with_guidance(curr_model, bert_tokenizer, dl_masks_train, num_epochs, lr, criterion, std_method, lmbda, device, weight_decay=weight_decay)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[std_method], f\"run_{i}.pt\"))\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_test, device)\n",
    "empty_model = llm.BertModelWrapper(1, device)\n",
    "avg_acc, wg_acc, wg, *_ = llm_restart_avg_and_worst_group_acc(\n",
    "    dl_masks_test, save_dir_for_method[std_method], empty_model, bert_tokenizer, device, num_groups=2\n",
    ")\n",
    "write_results_to_file(\"experiment_results/imdb_decoy_bert.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg}, std_method)\n",
    "write_results_to_file(\"experiment_results/imdb_decoy_bert_params.yaml\",\n",
    "                        {\"k\": lmbda,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": False}, std_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Training model with method r3 restart 0 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "Epoch:   0%|          | 0/1 [01:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m curr_model \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mBertModelWrapper(\u001b[38;5;241m1\u001b[39m, device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m========== Training model with method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr3_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m restart \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ==========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_llm_with_guidance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_masks_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr3_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlmbda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_fragments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting model accuracy for the training set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/R4/examples/models/pipeline.py:255\u001b[0m, in \u001b[0;36mtrain_llm_with_guidance\u001b[0;34m(model, tokenizer, dl_train, n_epochs, learning_rate, criterion, mlx_method, lmbda, device, num_fragments, alpha, num_samples, weight_decay, class_weights, suppress_tqdm)\u001b[0m\n\u001b[1;32m    253\u001b[0m std_loss \u001b[38;5;241m=\u001b[39m criterion(output_fragment, labels_fragment)\n\u001b[1;32m    254\u001b[0m fragment_loss \u001b[38;5;241m=\u001b[39m std_loss \u001b[38;5;241m+\u001b[39m lmbda \u001b[38;5;241m*\u001b[39m inp_grad_reg\n\u001b[0;32m--> 255\u001b[0m \u001b[43mfragment_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m batch_std_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m std_loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    257\u001b[0m batch_reg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m inp_grad_reg\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/fypvenv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/fypvenv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/mg2720/fypvenv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(models.pipeline)\n",
    "reload(models.robust_regularizer)\n",
    "r3_method = \"r3\"\n",
    "# hyperparameters\n",
    "num_epochs, lr, restarts, weight_decay, lmbda, num_frags = 1, 0.00001, 1, 1e-5, 2e+12, 4\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "train_acc, test_acc = 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + seed)\n",
    "    curr_model = llm.BertModelWrapper(1, device)\n",
    "\n",
    "    print(f\"========== Training model with method {r3_method} restart {i} ==========\")\n",
    "    train_llm_with_guidance(\n",
    "        curr_model, bert_tokenizer, dl_masks_train, num_epochs, lr, criterion, r3_method, lmbda, device, num_fragments=num_frags, weight_decay=weight_decay\n",
    "    )\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_test, device)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[r3_method], f\"run_{i}.pt\"))\n",
    "empty_model = llm.BertModelWrapper(1, device)\n",
    "avg_acc, wg_acc, wg, *_ = llm_restart_avg_and_worst_group_acc(\n",
    "    dl_masks_test, save_dir_for_method[r3_method], empty_model, bert_tokenizer, device, num_groups=2\n",
    ")\n",
    "write_results_to_file(\"experiment_results/imdb_decoy_bert.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg}, r3_method)\n",
    "write_results_to_file(\"experiment_results/imdb_decoy_bert_params.yaml\",\n",
    "                        {\"k\": lmbda,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": False}, r3_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth-R3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(models.pipeline)\n",
    "reload(models.robust_regularizer)\n",
    "smooth_r3_method = \"smooth_r3\"\n",
    "# hyperparameters\n",
    "num_epochs, lr, restarts, weight_decay, lmbda, num_frags, n_samples, alpha = 2, 0.00005, 1, 0, 1e+9, 11, 3, 0.1\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "train_acc, test_acc = 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + seed)\n",
    "    curr_model = llm.BertModelWrapper(1, device)\n",
    "\n",
    "    print(f\"========== Training model with method {smooth_r3_method} restart {i} ==========\")\n",
    "    train_llm_with_guidance(\n",
    "        curr_model, bert_tokenizer, dl_masks_train, num_epochs, lr, criterion, smooth_r3_method, lmbda, device,\n",
    "        num_fragments=num_frags, weight_decay=weight_decay, num_samples=n_samples, alpha=alpha\n",
    "    )\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_test, device)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[smooth_r3_method], f\"run_{i}.pt\"))\n",
    "empty_model = llm.BertModelWrapper(1, device)\n",
    "avg_acc, wg_acc, wg, *_ = llm_restart_avg_and_worst_group_acc(\n",
    "    dl_masks_test, save_dir_for_method[smooth_r3_method], empty_model, bert_tokenizer, device, num_groups=2\n",
    ")\n",
    "write_results_to_file(\"experiment_results/imdb_decoy_bert.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg}, smooth_r3_method)\n",
    "write_results_to_file(\"experiment_results/imdb_decoy_bert_params.yaml\",\n",
    "                        {\"k\": lmbda,\n",
    "                         \"alpha\": alpha,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"num_samples\": n_samples,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": False}, smooth_r3_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rand-R4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(models.pipeline)\n",
    "reload(models.robust_regularizer)\n",
    "rand_r4_method = \"rand_r4\"\n",
    "# hyperparameters\n",
    "num_epochs, lr, restarts, weight_decay, lmbda, num_frags, n_samples, alpha = 1, 0.00003, 1, 0, 8e+11, 11, 3, 0.75\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "train_acc, test_acc = 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + seed)\n",
    "    curr_model = llm.BertModelWrapper(1, device)\n",
    "\n",
    "    print(f\"========== Training model with method {rand_r4_method} restart {i} ==========\")\n",
    "    train_llm_with_guidance(\n",
    "        curr_model, bert_tokenizer, dl_masks_train, num_epochs, lr, criterion, rand_r4_method, lmbda, device,\n",
    "        num_fragments=num_frags, weight_decay=weight_decay, num_samples=n_samples, alpha=alpha\n",
    "    )\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_test, device)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[rand_r4_method], f\"run_{i}.pt\"))\n",
    "empty_model = llm.BertModelWrapper(1, device)\n",
    "avg_acc, wg_acc, wg, *_ = llm_restart_avg_and_worst_group_acc(\n",
    "    dl_masks_test, save_dir_for_method[rand_r4_method], empty_model, bert_tokenizer, device, num_groups=2\n",
    ")\n",
    "write_results_to_file(\"experiment_results/imdb_decoy_bert.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg}, rand_r4_method)\n",
    "write_results_to_file(\"experiment_results/imdb_decoy_bert_params.yaml\",\n",
    "                        {\"k\": lmbda,\n",
    "                         \"alpha\": alpha,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"num_samples\": n_samples,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": False}, rand_r4_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCG Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(models.pipeline)\n",
    "reload(models.robust_regularizer)\n",
    "pgd_r4_method = \"pgd_r4\"\n",
    "# hyperparameters\n",
    "num_epochs, lr, restarts, weight_decay, lmbda, num_frags, alpha = 1, 0.00005, 1, 0, 5e+10, 6, 0.3\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "train_acc, test_acc = 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + seed)\n",
    "    curr_model = llm.BertModelWrapper(1, device)\n",
    "\n",
    "    print(f\"========== Training model with method {pgd_r4_method} restart {i} ==========\")\n",
    "    train_llm_with_guidance(\n",
    "        curr_model, bert_tokenizer, dl_masks_train, num_epochs, lr, criterion, pgd_r4_method,\n",
    "        lmbda, device, num_fragments=num_frags, weight_decay=weight_decay\n",
    "    )\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_test, device)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[pgd_r4_method], f\"run_{i}.pt\"))\n",
    "empty_model = llm.BertModelWrapper(1, device)\n",
    "avg_acc, wg_acc, wg, *_ = llm_restart_avg_and_worst_group_acc(\n",
    "    dl_masks_test, save_dir_for_method[pgd_r4_method], empty_model, bert_tokenizer, device, num_groups=2\n",
    ")\n",
    "write_results_to_file(\"experiment_results/imdb_decoy_bert.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg}, pgd_r4_method)\n",
    "write_results_to_file(\"experiment_results/imdb_decoy_bert_params.yaml\",\n",
    "                        {\"k\": lmbda,\n",
    "                         \"alpha\": alpha,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": False}, pgd_r4_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
