{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certified Finetuning of a Classifier on the OCT-MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import tqdm\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training import certified_training_utils as ct_utils\n",
    "from models.deepmind import DeepMindSmall \n",
    "from datasets import oct_mnist\n",
    "from models.robust_regularizer import parameter_gradient_interval_regularizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the robustness of a non-robustly pre-trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of non-robustly trained classifier on test set with epsilon=0.01: [0.00, 0.96, 1.00]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "_, dl_test = oct_mnist.get_dataloaders(1000, exclude_classes=[2], balanced=True)\n",
    "standard_model = DeepMindSmall(1, 1).to(device)\n",
    "standard_model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n",
    "params_l, params_n, params_u = ct_utils.get_parameters(standard_model)\n",
    "epsilon = 0.01\n",
    "test_batch, test_labels = next(iter(dl_test))\n",
    "accs = agt.test_metrics.test_accuracy(\n",
    "    params_l, params_n, params_u, test_batch, test_labels, standard_model, ct_utils.propagate_conv_layers, epsilon\n",
    ")\n",
    "accs = \", \".join([f\"{a:.2f}\" for a in accs])\n",
    "\n",
    "print(f\"Accuracy of non-robustly trained classifier on test set with epsilon={epsilon}: [{accs}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train the model\n",
    "\n",
    "Exclude class 2 (Drusen) from the pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pre-training\n",
    "torch.manual_seed(1)\n",
    "pretrain_batchsize = 100\n",
    "pretrain_n_epochs = 10\n",
    "pretrain_learning_rate = 0.001\n",
    "pretrain_epsilon = 0.55\n",
    "pretrain_model_epsilon = 0.001\n",
    "pretrain_reg_strength = 0.4\n",
    "model_path = f\".models/medmnist_robust_eps{pretrain_epsilon}_alpha{pretrain_reg_strength}_meps{pretrain_model_epsilon}.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, dataset and optimizer\n",
    "model = DeepMindSmall(1, 1)\n",
    "dl_pretrain, _ = oct_mnist.get_dataloaders(pretrain_batchsize, exclude_classes=[2], balanced=True)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_learning_rate)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [03:08<00:00, 18.81s/it, bce_loss=0.384, loss=0.479, reg=0.238]\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:  # pre-train the model\n",
    "    progress_bar = tqdm.trange(pretrain_n_epochs, desc=\"Epoch\")\n",
    "    for epoch in progress_bar:\n",
    "        for i, (x, u) in enumerate(dl_pretrain):\n",
    "            # Forward pass\n",
    "            u, x = u.to(device), x.to(device)\n",
    "            output = model(x)\n",
    "            bce_loss = criterion(output.squeeze().float(), u.squeeze().float())\n",
    "            regularization = parameter_gradient_interval_regularizer(\n",
    "                model, x, u, \"binary_cross_entropy\", pretrain_epsilon, pretrain_model_epsilon\n",
    "            )\n",
    "            loss = bce_loss + pretrain_reg_strength * regularization\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                progress_bar.set_postfix(loss=loss.item(), bce_loss=bce_loss.item(), reg=regularization.item())\n",
    "    # save the model\n",
    "    with open(model_path, \"wb\") as file:\n",
    "        torch.save(model.state_dict(), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the robustness of the model pre-trained with the gradient interval regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".models/medmnist_robust_eps0.55_alpha0.4_meps0.001.ckpt 0.30, 0.85, 0.99\n",
      "Accuracy of robustly trained classifier on test set with epsilon=0.01: [0.30, 0.85, 0.99]\n"
     ]
    }
   ],
   "source": [
    "_, dl_test = oct_mnist.get_dataloaders(1000, exclude_classes=[2], balanced=True)\n",
    "params_l, params_n, params_u = ct_utils.get_parameters(model)\n",
    "test_batch, test_labels = next(iter(dl_test))\n",
    "accs = agt.test_metrics.test_accuracy(\n",
    "    params_l, params_n, params_u, test_batch, test_labels, model, ct_utils.propagate_conv_layers, epsilon=0.01\n",
    ")\n",
    "accs = \", \".join([f\"{a:.2f}\" for a in accs])\n",
    "print(model_path, accs)\n",
    "print(f\"Accuracy of robustly trained classifier on test set with epsilon={epsilon}: [{accs}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "medmnist_robust_eps0.01_alpha0.5_meps0.005.ckpt: 0.21, 0.94, 1.0\n",
    "\n",
    "medmnist_robust_eps0.01_alpha1.0_meps0.005.ckpt: 0.21, 0.92, 1.0\n",
    "\n",
    "medmnist_robust_eps0.05_alpha0.1_meps0.01.ckpt 0.02, 0.95, 1.0\n",
    "\n",
    "medmnist_robust_eps0.04_alpha0.1_meps0.001.ckpt 0.00, 0.95, 1.00\n",
    "\n",
    "medmnist_robust_eps0.1_alpha0.1_meps0.001.ckpt 0.00, 0.94, 1.00\n",
    "\n",
    "medmnist_robust_eps0.5_alpha0.1_meps0.001.ckpt 0.14, 0.93, 1.00\n",
    "\n",
    "medmnist_robust_eps0.5_alpha0.2_meps0.001.ckpt 0.21, 0.88, 1.00\n",
    "\n",
    "medmnist_robust_eps0.5_alpha0.3_meps0.001.ckpt 0.32, 0.82, 0.98\n",
    "\n",
    "medmnist_robust_eps0.5_alpha0.4_meps0.001.ckpt 0.29, 0.85, 0.99\n",
    "\n",
    "medmnist_robust_eps0.5_alpha0.5_meps0.001.ckpt 0.33, 0.33, 0.33\n",
    "\n",
    "medmnist_robust_eps0.45_alpha0.5_meps0.001.ckpt 0.30, 0.75, 0.99\n",
    "\n",
    "medmnist_robust_eps0.55_alpha0.4_meps0.001.ckpt 0.31, 0.84, 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "Include all classes, only allowing class 2 (Drusen) to be potentially poisoned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up fine-tuning parameters\n",
    "clean_batchsize = 3000\n",
    "drusen_batchsize = 3000\n",
    "test_batchsize = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=========== Pre-trained model accuracy ===========\n",
      "Class 2 (Drusen) : nominal = 0.39\n",
      "Classes 0, 1, 3  : nominal = 0.95\n",
      "All Classes      : nominal = 0.81\n",
      "[AGT] [INFO    ] [16:21:53] =================== Starting Poison Certified Training ===================\n",
      "[AGT] [DEBUG   ] [16:21:53] \tAdversary budget: epsilon=0.01, k_poison=50, label_epsilon=0, label_k_poison=0\n",
      "[AGT] [DEBUG   ] [16:21:53] \tBounding methods: forward=interval, backward=interval\n",
      "[AGT] [INFO    ] [16:21:53] Starting epoch 1\n",
      "[AGT] [DEBUG   ] [16:21:53] Initialising dataloader batchsize to 6000\n",
      "[AGT] [INFO    ] [16:21:53] Training batch 1: Network eval bounds=(0.39, 0.39, 0.39), W0 Bound=0.0 \n",
      "[AGT] [INFO    ] [16:21:54] Training batch 2: Network eval bounds=(0.85, 0.91, 0.92), W0 Bound=0.0163 \n",
      "[AGT] [DEBUG   ] [16:21:55] Skipping batch 3 in epoch 1 (expected batchsize 6000, got 4754)\n",
      "[AGT] [INFO    ] [16:21:55] Starting epoch 2\n",
      "[AGT] [INFO    ] [16:21:55] Training batch 3: Network eval bounds=(0.83, 0.9 , 0.93), W0 Bound=0.0214 \n",
      "[AGT] [INFO    ] [16:21:56] Training batch 4: Network eval bounds=(0.83, 0.9 , 0.94), W0 Bound=0.0251 \n",
      "[AGT] [DEBUG   ] [16:21:57] Skipping batch 3 in epoch 2 (expected batchsize 6000, got 4754)\n",
      "[AGT] [INFO    ] [16:21:57] Final network eval: Network eval bounds=(0.82, 0.89, 0.94), W0 Bound=0.0281 \n",
      "[AGT] [INFO    ] [16:21:57] =================== Finished Poison Certified Training ===================\n",
      "=========== Fine-tuned model accuracy + bounds ===========\n",
      "Class 2 (Drusen) : nominal = 0.89, certified bound = 0.82\n",
      "Classes 0, 1, 3  : nominal = 0.83, certified bound = 0.77\n",
      "All Classes      : nominal = 0.85, certified bound = 0.78\n"
     ]
    }
   ],
   "source": [
    "from abstract_gradient_training.poisoning import poison_certified_training\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# get dataloaders\n",
    "dl_clean, dl_test_clean = oct_mnist.get_dataloaders(clean_batchsize, test_batchsize, exclude_classes=[2])\n",
    "dl_drusen, dl_test_drusen = oct_mnist.get_dataloaders(drusen_batchsize, test_batchsize, exclude_classes=[0, 1, 3])\n",
    "_, dl_test_all = oct_mnist.get_dataloaders(clean_batchsize, test_batchsize)\n",
    "\n",
    "# evaluate the pre-trained model\n",
    "param_n, param_l, param_u = ct_utils.get_parameters(model)\n",
    "drusen_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_drusen)), model, ct_utils.propagate_conv_layers)\n",
    "clean_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_clean)), model, ct_utils.propagate_conv_layers)\n",
    "all_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers)\n",
    "\n",
    "print(\"=========== Pre-trained model accuracy ===========\", file=sys.stderr)\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}\", file=sys.stderr)\n",
    "print(f\"Classes 0, 1, 3  : nominal = {clean_acc[1]:.2g}\", file=sys.stderr)\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}\", file=sys.stderr)\n",
    "\n",
    "config = AGTConfig(\n",
    "    fragsize = 2000,\n",
    "    learning_rate = 0.06,\n",
    "    n_epochs = 2,\n",
    "    k_poison = 50,\n",
    "    epsilon = 0.01,\n",
    "    # clip_gamma = 2.0,\n",
    "    forward_bound = \"interval\",\n",
    "    device = \"cuda:1\",\n",
    "    backward_bound = \"interval\",\n",
    "    loss = \"binary_cross_entropy\",\n",
    "    log_level=\"DEBUG\",\n",
    "    lr_decay=4.0,\n",
    "    lr_min=0.001,\n",
    ")\n",
    "\n",
    "# fine-tune the model using abstract gradient training (keeping the convolutional layers fixed)\n",
    "param_l, param_n, param_u = poison_certified_training(\n",
    "    model, config, dl_drusen, dl_test_drusen, dl_clean=dl_clean, transform=ct_utils.propagate_conv_layers\n",
    ")\n",
    "\n",
    "# evaluate the fine-tuned model\n",
    "drusen_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_drusen)), model, ct_utils.propagate_conv_layers)\n",
    "clean_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_clean)), model, ct_utils.propagate_conv_layers)\n",
    "all_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers)\n",
    "\n",
    "print(\"=========== Fine-tuned model accuracy + bounds ===========\", file=sys.stderr)\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}, certified bound = {drusen_acc[0]:.2g}\", file=sys.stderr)\n",
    "print(f\"Classes 0, 1, 3  : nominal = {clean_acc[1]:.2g}, certified bound = {clean_acc[0]:.2g}\", file=sys.stderr)\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}, certified bound = {all_acc[0]:.2g}\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=========== Pre-trained model accuracy ===========\n",
      "Class 2 (Drusen) : nominal = 0.39\n",
      "Classes 0, 1, 3  : nominal = 0.95\n",
      "All Classes      : nominal = 0.81\n",
      "[AGT] [INFO    ] [16:19:21] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [16:19:21] Starting epoch 1\n",
      "[AGT] [INFO    ] [16:19:21] Training batch 1: Network eval bounds=(0.39, 0.39, 0.39), W0 Bound=0.0 \n",
      "[AGT] [INFO    ] [16:19:22] Training batch 2: Network eval bounds=(0.84, 0.97, 1   ), W0 Bound=3.77 \n",
      "[AGT] [INFO    ] [16:19:22] Starting epoch 2\n",
      "[AGT] [INFO    ] [16:19:23] Training batch 3: Network eval bounds=(0.7 , 0.94, 1   ), W0 Bound=4.53 \n",
      "[AGT] [INFO    ] [16:19:23] Training batch 4: Network eval bounds=(0.58, 0.92, 1   ), W0 Bound=4.95 \n",
      "[AGT] [INFO    ] [16:19:24] Final network eval: Network eval bounds=(0.49, 0.92, 1   ), W0 Bound=5.24 \n",
      "[AGT] [INFO    ] [16:19:24] =================== Finished Privacy Certified Training ===================\n",
      "=========== Fine-tuned model accuracy + bounds ===========\n",
      "Class 2 (Drusen) : nominal = 0.92, certified bound = 0.49\n",
      "Classes 0, 1, 3  : nominal = 0.8, certified bound = 0.65\n",
      "All Classes      : nominal = 0.83, certified bound = 0.61\n"
     ]
    }
   ],
   "source": [
    "from abstract_gradient_training.privacy import privacy_certified_training\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# get dataloaders\n",
    "dl_clean, dl_test_clean = oct_mnist.get_dataloaders(clean_batchsize, test_batchsize, exclude_classes=[2])\n",
    "dl_drusen, dl_test_drusen = oct_mnist.get_dataloaders(drusen_batchsize, test_batchsize, exclude_classes=[0, 1, 3])\n",
    "_, dl_test_all = oct_mnist.get_dataloaders(clean_batchsize, test_batchsize)\n",
    "\n",
    "# evaluate the pre-trained model\n",
    "param_n, param_l, param_u = ct_utils.get_parameters(model)\n",
    "drusen_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_drusen)), model, ct_utils.propagate_conv_layers)\n",
    "clean_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_clean)), model, ct_utils.propagate_conv_layers)\n",
    "all_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers)\n",
    "\n",
    "print(\"=========== Pre-trained model accuracy ===========\", file=sys.stderr)\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}\", file=sys.stderr)\n",
    "print(f\"Classes 0, 1, 3  : nominal = {clean_acc[1]:.2g}\", file=sys.stderr)\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}\", file=sys.stderr)\n",
    "\n",
    "config = AGTConfig(\n",
    "    fragsize = 500,\n",
    "    learning_rate = 0.08,\n",
    "    n_epochs = 2,\n",
    "    k_private = 50,\n",
    "    forward_bound = \"interval\",\n",
    "    device = \"cuda:0\",\n",
    "    clip_gamma = 5.0,\n",
    "    backward_bound = \"interval\",\n",
    "    loss = \"binary_cross_entropy\",\n",
    "    lr_decay=4.0,\n",
    "    lr_min=0.001,\n",
    ")\n",
    "\n",
    "# fine-tune the model using abstract gradient training (keeping the convolutional layers fixed)\n",
    "param_l, param_n, param_u = privacy_certified_training(\n",
    "    model, config, dl_drusen, dl_test_drusen, dl_public=dl_clean, transform=ct_utils.propagate_conv_layers\n",
    ")\n",
    "\n",
    "# evaluate the fine-tuned model\n",
    "drusen_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_drusen)), model, ct_utils.propagate_conv_layers)\n",
    "clean_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_clean)), model, ct_utils.propagate_conv_layers)\n",
    "all_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers)\n",
    "\n",
    "print(\"=========== Fine-tuned model accuracy + bounds ===========\", file=sys.stderr)\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}, certified bound = {drusen_acc[0]:.2g}\", file=sys.stderr)\n",
    "print(f\"Classes 0, 1, 3  : nominal = {clean_acc[1]:.2g}, certified bound = {clean_acc[0]:.2g}\", file=sys.stderr)\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}, certified bound = {all_acc[0]:.2g}\", file=sys.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
