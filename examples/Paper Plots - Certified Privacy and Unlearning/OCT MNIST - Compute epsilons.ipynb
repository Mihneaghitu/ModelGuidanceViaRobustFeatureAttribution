{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Certified privacy- and unlearning-safe training on the OCT-MNIST dataset\n",
    "\n",
    "Run over different privacy and unlearning parameters and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import opacus\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import unlearning\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training import certified_training_utils as ct_utils\n",
    "from abstract_gradient_training import test_metrics\n",
    "\n",
    "sys.path.append('..')\n",
    "from models.deepmind import DeepMindSmall \n",
    "from datasets import oct_mnist\n",
    "\n",
    "# opacus doesn't respect my logging handler :(\n",
    "logger = logging.getLogger(\"abstract_gradient_training\")\n",
    "logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon_delta(\n",
    "    config: agt.AGTConfig,\n",
    "    dl_train: torch.utils.data.DataLoader,\n",
    "    delta: float = 10e-5,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    For the given model and abstract gradient training config, compute the equivalent epsilon and delta values using\n",
    "    opacus.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Sequential): Neural network model. Must be a torch.nn.Sequential object with dense layers and\n",
    "            ReLU activations only. The model may have other layers (e.g. convolutional layers) before the dense section,\n",
    "            but these must be fixed and are not trained. If fixed non-dense layers are provided, then the transform\n",
    "            function must be set to propagate bounds through these layers.\n",
    "        config (AGTConfig): Configuration object for the abstract gradient training module. See the configuration module\n",
    "            for more details.\n",
    "        dl_train (DataLoader): Training data loader.\n",
    "        delta (float, optional): Desired delta value for the privacy calculation. Defaults to 10e-5.\n",
    "\n",
    "    Returns:\n",
    "        float: The epsilon value for the given model and config.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the config variables\n",
    "    device = config.device\n",
    "    lr_decay = config.lr_decay\n",
    "    lr_min = config.lr_min\n",
    "    dp_sgd_sigma = config.dp_sgd_sigma\n",
    "    clipping = config.clip_gamma\n",
    "    learning_rate = config.learning_rate\n",
    "    n_epochs = config.n_epochs\n",
    "    batchsize = next(iter(dl_train))[0].size(0)\n",
    "    model = DeepMindSmall(1, 1).to(device)\n",
    "\n",
    "    if config.loss == \"binary_cross_entropy\":\n",
    "        criterion = torch.nn.BCELoss()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Loss function {config.loss} not implemented for eps-delta calculation.\")\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    privacy_engine = opacus.PrivacyEngine(accountant=\"rdp\")\n",
    "    model_private, optimizer_private, data_loader_private = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=dl_train,\n",
    "        noise_multiplier=dp_sgd_sigma,\n",
    "        max_grad_norm=clipping,\n",
    "        poisson_sampling=False,\n",
    "    )\n",
    "\n",
    "    def get_lr(epoch):\n",
    "        lr = max(1 / (1 + lr_decay * epoch), lr_min / learning_rate)\n",
    "        return lr\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer_private, get_lr)\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        for i, (x, u) in enumerate(dl_train):\n",
    "            # AGT only takes full batches\n",
    "            if u.size(0) < batchsize:\n",
    "                break\n",
    "            u, x = u.to(device), x.to(device)\n",
    "            output = model_private(x)\n",
    "            loss = criterion(output.squeeze().float(), u.squeeze().float())\n",
    "            # Backward and optimize\n",
    "            optimizer_private.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_private.step()\n",
    "            scheduler.step()\n",
    "\n",
    "    # compute privacy guarantees\n",
    "    epsilon = privacy_engine.accountant.get_epsilon(delta=delta)\n",
    "    return epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting options\n",
    "sns.set_theme(context=\"poster\", style=\"whitegrid\", font_scale=1.7)\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "palette = [\"#DDC4DD\", \"#DCCFEC\", \"#A997DF\", \"#4F517D\", \"#1A3A3A\"][::-1]\n",
    "fontsize = \"large\"\n",
    "seed = 2\n",
    "results_dir = \".results/\"\n",
    "notebook_id = f\"oct_sweep_v4_{seed}\"\n",
    "model_path = \".models/medmnist.ckpt\"  # pretrained model path\n",
    "draft = True  # whether to compute the full suite of results or a quicker reduced version\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the nominal config, model and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 5000\n",
    "nominal_config = AGTConfig(\n",
    "    fragsize=2000,\n",
    "    learning_rate=0.2,\n",
    "    n_epochs=2,\n",
    "    forward_bound=\"interval\",\n",
    "    device=\"cuda:1\",\n",
    "    backward_bound=\"interval\",\n",
    "    loss=\"binary_cross_entropy\",\n",
    "    log_level=\"DEBUG\",\n",
    "    lr_decay=2.0,\n",
    "    # dp_sgd_sigma=1.0,\n",
    "    lr_min=0.001,\n",
    "    early_stopping=False,\n",
    "    metadata=f\"model={model_path}\"\n",
    ")\n",
    "\n",
    "\n",
    "# get the \"DeepMindSmall\" model, pretrained on the MedMNIST dataset (without class 2, Drusen)\n",
    "model = DeepMindSmall(1, 1)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.to(nominal_config.device)\n",
    "\n",
    "# get dataloaders, train dataloader is a mix of drusen and the \"healthy\" class\n",
    "dl_train, _ = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1], balanced=True)\n",
    "_, dl_test_drusen = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1, 3])\n",
    "_, dl_test_other = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[2])\n",
    "_, dl_test_all = oct_mnist.get_dataloaders(batchsize, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-trained accuracy on Drusen: 0.456\n",
      "Pre-trained accuracy on all classes:  0.835\n",
      "Pre-trained accuracy on classes 0, 1, 3:  0.961\n",
      "09/28/2024 03:51:19:INFO:=================== Starting Privacy Certified Training ===================\n",
      "09/28/2024 03:51:19:DEBUG:\tOptimizer params: n_epochs=2, learning_rate=0.2, l1_reg=0.0, l2_reg=0.0\n",
      "09/28/2024 03:51:19:DEBUG:\tLearning rate schedule: lr_decay=2.0, lr_min=0.001, early_stopping=False\n",
      "09/28/2024 03:51:19:DEBUG:\tPrivacy parameter: k_private=1\n",
      "09/28/2024 03:51:19:DEBUG:\tClipping: gamma=0.5, method=clamp\n",
      "09/28/2024 03:51:19:DEBUG:\tNoise: type=gaussian, sigma=0.1\n",
      "09/28/2024 03:51:19:DEBUG:\tBounding methods: forward=interval, loss=binary_cross_entropy, backward=interval\n",
      "09/28/2024 03:51:19:DEBUG:\tUsing Gaussian privacy-preserving noise (std 0.05)\n",
      "09/28/2024 03:51:19:INFO:Starting epoch 1\n",
      "09/28/2024 03:51:19:DEBUG:Initialising dataloader batchsize to 5000\n",
      "09/28/2024 03:51:19:INFO:Training batch 1: Network eval bounds=(0.46, 0.46, 0.46), W0 Bound=0.0 \n",
      "09/28/2024 03:51:20:INFO:Training batch 2: Network eval bounds=(0.66, 0.66, 0.66), W0 Bound=0.0254 \n",
      "09/28/2024 03:51:21:INFO:Training batch 3: Network eval bounds=(0.7 , 0.71, 0.71), W0 Bound=0.0346 \n",
      "09/28/2024 03:51:22:DEBUG:Skipping batch 4 in epoch 1 (expected batchsize 5000, got 508)\n",
      "09/28/2024 03:51:22:INFO:Starting epoch 2\n",
      "09/28/2024 03:51:22:INFO:Training batch 4: Network eval bounds=(0.72, 0.72, 0.72), W0 Bound=0.0403 \n",
      "09/28/2024 03:51:23:INFO:Training batch 5: Network eval bounds=(0.72, 0.73, 0.74), W0 Bound=0.0446 \n",
      "09/28/2024 03:51:24:INFO:Training batch 6: Network eval bounds=(0.75, 0.77, 0.78), W0 Bound=0.048 \n",
      "09/28/2024 03:51:24:DEBUG:Skipping batch 4 in epoch 2 (expected batchsize 5000, got 508)\n",
      "09/28/2024 03:51:24:INFO:Final network eval: Network eval bounds=(0.77, 0.78, 0.78), W0 Bound=0.0509 \n",
      "09/28/2024 03:51:24:INFO:=================== Finished Privacy Certified Training ===================\n",
      "Fine-tuned accuracy on Drusen: 0.776\n",
      "Fine-tuned accuracy on all classes: 0.886\n",
      "Fine-tuned accuracy on classes 0, 1, 3: 0.923\n",
      "Fine-tuned percent certified on all classes: 0.994\n",
      "/vol/bitbucket/ps1623/venv/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/vol/bitbucket/ps1623/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328.0724533098246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/ps1623/venv/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the smallest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# compute pre-trained model accuracy\n",
    "pretrain_acc = test_metrics.test_accuracy(\n",
    "    *ct_utils.get_parameters(model), *next(iter(dl_test_drusen)), model, ct_utils.propagate_conv_layers\n",
    ")[1]\n",
    "pretrain_acc_all = test_metrics.test_accuracy(\n",
    "    *ct_utils.get_parameters(model), *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers\n",
    ")[1]\n",
    "pretrain_acc_other = test_metrics.test_accuracy(\n",
    "    *ct_utils.get_parameters(model), *next(iter(dl_test_other)), model, ct_utils.propagate_conv_layers\n",
    ")[1]\n",
    "print(f\"Pre-trained accuracy on Drusen: {pretrain_acc:.3g}\", file=sys.stderr)\n",
    "print(f\"Pre-trained accuracy on all classes:  {pretrain_acc_all:.3g}\", file=sys.stderr)\n",
    "print(f\"Pre-trained accuracy on classes 0, 1, 3:  {pretrain_acc_other:.3g}\", file=sys.stderr)\n",
    "\n",
    "# perform one certified training run with to check the nominal accuracy we get\n",
    "config = copy.deepcopy(nominal_config)\n",
    "config.k_private = 1\n",
    "config.clip_gamma = 0.5\n",
    "config.dp_sgd_sigma = 0.1\n",
    "torch.manual_seed(seed)\n",
    "param_l, param_n, param_u = agt.privacy_certified_training(\n",
    "    model, config, dl_train, dl_test_drusen, transform=ct_utils.propagate_conv_layers\n",
    ")\n",
    "\n",
    "# compute the accuracies of the fine-tuned model\n",
    "finetune_acc = test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_drusen)), model, ct_utils.propagate_conv_layers\n",
    ")[1]\n",
    "finetune_acc_all = test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers\n",
    ")[1]\n",
    "finetune_acc_other = test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_other)), model, ct_utils.propagate_conv_layers\n",
    ")[1]\n",
    "print(f\"Fine-tuned accuracy on Drusen: {finetune_acc:.3g}\",file=sys.stderr)\n",
    "print(f\"Fine-tuned accuracy on all classes: {finetune_acc_all:.3g}\",file=sys.stderr)\n",
    "print(f\"Fine-tuned accuracy on classes 0, 1, 3: {finetune_acc_other:.3g}\", file=sys.stderr)\n",
    "\n",
    "percent_certified = test_metrics.proportion_certified(\n",
    "    param_n, param_l, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers\n",
    ")\n",
    "print(f\"Fine-tuned percent certified on all classes: {percent_certified:.3g}\", file=sys.stderr)\n",
    "\n",
    "del param_l, param_n, param_u\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(get_epsilon_delta(config, dl_train, delta=10e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_config(config):\n",
    "    \"\"\"If results for this configuration are already computed, load them from disk. Otherwise, run the certified\n",
    "    training using AGT, then save and return the results.\"\"\"\n",
    "    fname = f\"{results_dir}/{notebook_id}_{config.hash()}\"\n",
    "    if os.path.isfile(fname):  # run exists, so return the previous results\n",
    "        param_l, param_n, param_u = torch.load(fname)\n",
    "    else:\n",
    "        # check whether the given config should be either unlearning or privacy training\n",
    "        assert not (config.k_unlearn and config.k_private)\n",
    "        torch.manual_seed(seed)\n",
    "        if config.k_private:\n",
    "            param_l, param_n, param_u = agt.privacy_certified_training(\n",
    "                model, config, dl_train, dl_test_drusen, transform=ct_utils.propagate_conv_layers\n",
    "            )\n",
    "        else:\n",
    "            param_l, param_n, param_u = agt.unlearning_certified_training(\n",
    "                model, config, dl_train, dl_test_drusen, transform=ct_utils.propagate_conv_layers\n",
    "            )\n",
    "        torch.save((param_l, param_n, param_u), fname)\n",
    "    # get nominal accuracy (on the Drusen class) and percent certified (on the entire test set)\n",
    "    param_l = [p.to(nominal_config.device) for p in param_l]\n",
    "    param_n = [p.to(nominal_config.device) for p in param_n]\n",
    "    param_u = [p.to(nominal_config.device) for p in param_u]\n",
    "    return param_l, param_n, param_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_gammas = [0.5, 1.0, 2.0]\n",
    "\n",
    "privacy_runs = {}\n",
    "\n",
    "for gamma in clip_gammas:\n",
    "    config = copy.deepcopy(nominal_config)\n",
    "    config.k_private = 1\n",
    "    config.clip_gamma = gamma\n",
    "    config.dp_sgd_sigma = 0.0\n",
    "    privacy_runs[gamma] = run_with_config(config)\n",
    "\n",
    "    # epsilons[sigma] = get_epsilon_delta(config, dl_train, delta=10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 10.0 &  0.5 & 88.1 $\\pm$ 0.18 & 99.7 & 0.03\\\\\n",
      "\t 10.0 &  1.0 & 87.3 $\\pm$ 0.19 & 98.1 & 0.19\\\\\n",
      "\t 10.0 &  2.0 & 87.2 $\\pm$ 0.16 & 97.0 & 0.3\\\\\n",
      "\t 1.0 &  0.5 & 65.1 $\\pm$ 1.3 & 99.7 & 0.003\\\\\n",
      "\t 1.0 &  1.0 & 64.6 $\\pm$ 1.5 & 98.1 & 0.019\\\\\n",
      "\t 1.0 &  2.0 & 64.5 $\\pm$ 1.4 & 97.0 & 0.03\\\\\n",
      "\t 0.5 &  0.5 & 58.6 $\\pm$ 1.6 & 99.7 & 0.0015\\\\\n",
      "\t 0.5 &  1.0 & 58.4 $\\pm$ 1.6 & 98.1 & 0.0095\\\\\n",
      "\t 0.5 &  2.0 & 58.2 $\\pm$ 1.6 & 97.0 & 0.015\\\\\n"
     ]
    }
   ],
   "source": [
    "epsilons = [10.0, 1.0, 0.5]\n",
    "n_runs = 100\n",
    "\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    # print(f\"============= {epsilon=} ===============\")\n",
    "\n",
    "    for gamma, (param_l, param_n, param_u) in privacy_runs.items():\n",
    "        # print accuracy\n",
    "        percent_certified = test_metrics.proportion_certified(\n",
    "            param_n, param_l, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers\n",
    "        )\n",
    "        private_accs = []\n",
    "        # avg over 100 runs\n",
    "        for _ in range(100):\n",
    "            private_accs.append(\n",
    "                test_metrics.test_accuracy(\n",
    "                    param_l, param_n, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers, noise_level=1 / epsilon\n",
    "                )[1]\n",
    "            )\n",
    "        mean_private_acc = np.mean(private_accs)\n",
    "        std_private_acc = np.std(private_accs)\n",
    "        \n",
    "        print(f\"\\t {epsilon} &  {gamma} & {100 * mean_private_acc:.1f} $\\pm$ {100 * std_private_acc:.2g} & {100*percent_certified:.1f} & {epsilon * (1 - percent_certified):.3g}\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DP-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon= 4.7142437673284485 clip_gamma 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:12<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.97600424289703 1.280400118718563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "# perform one certified training run with to check the nominal accuracy we get\n",
    "config = copy.deepcopy(nominal_config)\n",
    "config.k_private = 1\n",
    "config.clip_gamma = 1.0\n",
    "config.dp_sgd_sigma = 1.0\n",
    "config.log_level = \"WARNING\"\n",
    "print(\"epsilon=\", get_epsilon_delta(config, dl_train, delta=10e-5), \"clip_gamma\", config.clip_gamma)\n",
    "\n",
    "private_accs = []\n",
    "n_runs = 50\n",
    "\n",
    "for _ in trange(n_runs):\n",
    "    param_l, param_n, param_u = agt.privacy_certified_training(\n",
    "        model, config, dl_train, dl_test_drusen, transform=ct_utils.propagate_conv_layers\n",
    "    )\n",
    "    private_accs.append(\n",
    "        test_metrics.test_accuracy(\n",
    "            param_l, param_n, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers\n",
    "        )[1]\n",
    "    )\n",
    "\n",
    "mean_private_acc = np.mean(private_accs)\n",
    "std_private_acc = np.std(private_accs)\n",
    "\n",
    "print(100 * mean_private_acc, 100 * std_private_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/ps1623/venv/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/vol/bitbucket/ps1623/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon= 4.7142437673284485 clip_gamma 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:12<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.89000368118286 1.6191668870743088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "# perform one certified training run with to check the nominal accuracy we get\n",
    "config = copy.deepcopy(nominal_config)\n",
    "config.k_private = 1\n",
    "config.clip_gamma = 2.0\n",
    "config.dp_sgd_sigma = 1.0\n",
    "config.log_level = \"WARNING\"\n",
    "print(\"epsilon=\", get_epsilon_delta(config, dl_train, delta=10e-5), \"clip_gamma\", config.clip_gamma)\n",
    "\n",
    "private_accs = []\n",
    "n_runs = 50\n",
    "\n",
    "for _ in trange(n_runs):\n",
    "    param_l, param_n, param_u = agt.privacy_certified_training(\n",
    "        model, config, dl_train, dl_test_drusen, transform=ct_utils.propagate_conv_layers\n",
    "    )\n",
    "    private_accs.append(\n",
    "        test_metrics.test_accuracy(\n",
    "            param_l, param_n, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers\n",
    "        )[1]\n",
    "    )\n",
    "\n",
    "mean_private_acc = np.mean(private_accs)\n",
    "std_private_acc = np.std(private_accs)\n",
    "\n",
    "print(100 * mean_private_acc, 100 * std_private_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/ps1623/venv/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the smallest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon= 328.0724533098246 clip_gamma 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:12<00:00,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.5560040473938 0.303420978474976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "# perform one certified training run with to check the nominal accuracy we get\n",
    "config = copy.deepcopy(nominal_config)\n",
    "config.k_private = 1\n",
    "config.clip_gamma = 1.0\n",
    "config.dp_sgd_sigma = 0.1\n",
    "config.log_level = \"WARNING\"\n",
    "print(\"epsilon=\", get_epsilon_delta(config, dl_train, delta=10e-5), \"clip_gamma\", config.clip_gamma)\n",
    "\n",
    "private_accs = []\n",
    "n_runs = 50\n",
    "\n",
    "for _ in trange(n_runs):\n",
    "    param_l, param_n, param_u = agt.privacy_certified_training(\n",
    "        model, config, dl_train, dl_test_drusen, transform=ct_utils.propagate_conv_layers\n",
    "    )\n",
    "    private_accs.append(\n",
    "        test_metrics.test_accuracy(\n",
    "            param_l, param_n, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers\n",
    "        )[1]\n",
    "    )\n",
    "\n",
    "mean_private_acc = np.mean(private_accs)\n",
    "std_private_acc = np.std(private_accs)\n",
    "\n",
    "print(100 * mean_private_acc, 100 * std_private_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon= 328.0724533098246 clip_gamma 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:12<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.3320038318634 0.4105803520599861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "# perform one certified training run with to check the nominal accuracy we get\n",
    "config = copy.deepcopy(nominal_config)\n",
    "config.k_private = 1\n",
    "config.clip_gamma = 2.0\n",
    "config.dp_sgd_sigma = 0.1\n",
    "config.log_level = \"WARNING\"\n",
    "print(\"epsilon=\", get_epsilon_delta(config, dl_train, delta=10e-5), \"clip_gamma\", config.clip_gamma)\n",
    "\n",
    "private_accs = []\n",
    "n_runs = 50\n",
    "\n",
    "for _ in trange(n_runs):\n",
    "    param_l, param_n, param_u = agt.privacy_certified_training(\n",
    "        model, config, dl_train, dl_test_drusen, transform=ct_utils.propagate_conv_layers\n",
    "    )\n",
    "    private_accs.append(\n",
    "        test_metrics.test_accuracy(\n",
    "            param_l, param_n, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers\n",
    "        )[1]\n",
    "    )\n",
    "\n",
    "mean_private_acc = np.mean(private_accs)\n",
    "std_private_acc = np.std(private_accs)\n",
    "\n",
    "print(100 * mean_private_acc, 100 * std_private_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epsilon = 328.0, clip = 0.5, acc = 88.1% +- 0.23%\n",
    "epsilon = 328.0, clip = 1.0, acc = 87.6% +- 0.30%\n",
    "epsilon = 328.0, clip = 2.0, acc = 87.3 +- 0.41\n",
    "\n",
    "epsilon = 4.71, clip = 0.5, acc = 87.5 +- 0.66\n",
    "epsilon = 4.71, clip = 1.0, acc = 85.98 +- 1.28\n",
    "epsilon = 4.71, clip = 2.0, acc = 81.89 +- 1.62\n",
    "\n",
    "epsilon = 1.53, clip = 0.5, acc = 85.72 +- 1.51\n",
    "epsilon = 1.53, clip = 1.0, acc = 81.5 +- 2.04\n",
    "epsilon = 1.53, clip = 2.0, acc = 74.1, +- 3.60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
