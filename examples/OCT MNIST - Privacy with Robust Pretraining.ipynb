{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certified Finetuning of a Classifier on the OCT-MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training import certified_training_utils as ct_utils\n",
    "from models.deepmind import DeepMindSmall \n",
    "from datasets import oct_mnist\n",
    "from models.robust_regularizer import parameter_gradient_interval_regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the robustness of a non-robustly pre-trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of non-robustly trained classifier on test set with epsilon=0.0005: [0.04, 0.96, 1.00]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "_, dl_test = oct_mnist.get_dataloaders(1000, exclude_classes=[2], balanced=True)\n",
    "standard_model = DeepMindSmall(1, 1).to(device)\n",
    "standard_model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n",
    "params_l, params_n, params_u = ct_utils.get_parameters(standard_model)\n",
    "epsilon = 0.0005\n",
    "test_batch, test_labels = next(iter(dl_test))\n",
    "accs = agt.test_metrics.test_accuracy(\n",
    "    params_l, params_n, params_u, test_batch, test_labels, standard_model, ct_utils.propagate_conv_layers, epsilon\n",
    ")\n",
    "accs = \", \".join([f\"{a:.2f}\" for a in accs])\n",
    "\n",
    "print(f\"Accuracy of non-robustly trained classifier on test set with epsilon={epsilon}: [{accs}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train the model\n",
    "\n",
    "Exclude class 2 (Drusen) from the pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pre-training\n",
    "torch.manual_seed(1)\n",
    "pretrain_batchsize = 100\n",
    "pretrain_n_epochs = 20\n",
    "pretrain_learning_rate = 0.001\n",
    "pretrain_epsilon = 0.0\n",
    "pretrain_model_epsilon = 1e-2\n",
    "pretrain_reg_strength = 0.05\n",
    "model_path = f\".models/medmnist_param_robust_eps{pretrain_epsilon}_alpha{pretrain_reg_strength}_meps{pretrain_model_epsilon}.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, dataset and optimizer\n",
    "model = DeepMindSmall(1, 1)\n",
    "dl_pretrain, _ = oct_mnist.get_dataloaders(pretrain_batchsize, exclude_classes=[2], balanced=True)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_learning_rate)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 10/20 [03:16<03:16, 19.61s/it, bce_loss=0.248, loss=0.34, reg=0.184, reg_strength=0.5]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kappa \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 15\u001b[0m     regularization \u001b[38;5;241m=\u001b[39m \u001b[43mparameter_gradient_interval_regularizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinary_cross_entropy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain_model_epsilon\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     regularization \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[0;32m/data/ps1623/AbstractGradientTrainingPrivate/examples/models/robust_regularizer.py:119\u001b[0m, in \u001b[0;36mparameter_gradient_interval_regularizer\u001b[0;34m(model, batch, labels, loss_fn, epsilon, model_epsilon, return_grads)\u001b[0m\n\u001b[1;32m    116\u001b[0m grads_l, grads_u \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(modules) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# compute the gradients wrt the parameters\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     gl, gu \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_module_parameter_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     grads_l\u001b[38;5;241m.\u001b[39mextend(gl)\n\u001b[1;32m    121\u001b[0m     grads_u\u001b[38;5;241m.\u001b[39mextend(gu)\n",
      "File \u001b[0;32m/data/ps1623/AbstractGradientTrainingPrivate/examples/models/robust_regularizer.py:275\u001b[0m, in \u001b[0;36mcompute_module_parameter_gradients\u001b[0;34m(module, dl_l, dl_u, x_l, x_u)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# compute gradients\u001b[39;00m\n\u001b[1;32m    274\u001b[0m dl_db_l, dl_db_u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(dl_l, dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)), torch\u001b[38;5;241m.\u001b[39msum(dl_u, dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m--> 275\u001b[0m dl_dW_l, dl_dW_u \u001b[38;5;241m=\u001b[39m \u001b[43minterval_arithmetic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate_linear_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# store the gradients\u001b[39;00m\n\u001b[1;32m    278\u001b[0m grads_l\u001b[38;5;241m.\u001b[39mextend([dl_db_l, dl_dW_l])\n",
      "File \u001b[0;32m/data/ps1623/AbstractGradientTrainingPrivate/abstract_gradient_training/interval_arithmetic.py:341\u001b[0m, in \u001b[0;36mpropagate_linear_transform\u001b[0;34m(A_l, A_u, B_l, B_u, transform)\u001b[0m\n\u001b[1;32m    338\u001b[0m B_r \u001b[38;5;241m=\u001b[39m (B_u \u001b[38;5;241m-\u001b[39m B_l) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# compute the \"mean\" and \"radius\" of the output\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m H_mu \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m H_r \u001b[38;5;241m=\u001b[39m transform(torch\u001b[38;5;241m.\u001b[39mabs(A_mu), B_r) \u001b[38;5;241m+\u001b[39m transform(A_r, torch\u001b[38;5;241m.\u001b[39mabs(B_mu)) \u001b[38;5;241m+\u001b[39m transform(A_r, B_r)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# convert to lower and upper bounds\u001b[39;00m\n",
      "File \u001b[0;32m/data/ps1623/AbstractGradientTrainingPrivate/examples/models/robust_regularizer.py:269\u001b[0m, in \u001b[0;36mcompute_module_parameter_gradients.<locals>.transform\u001b[0;34m(x, dl)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(x, dl):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilation\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/ps1623/venv/lib/python3.10/site-packages/torch/nn/grad.py:126\u001b[0m, in \u001b[0;36mconv2d_weight\u001b[0;34m(input, weight_size, grad_output, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconv2d_weight\u001b[39m(\u001b[38;5;28minput\u001b[39m, weight_size, grad_output, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    Computes the gradient of conv2d with respect to the weight of the convolution.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m \n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_empty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39mconvolution_backward(grad_output, \u001b[38;5;28minput\u001b[39m, weight, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    129\u001b[0m                                                _pair(stride), _pair(padding), _pair(dilation),\n\u001b[1;32m    130\u001b[0m                                                \u001b[38;5;28;01mFalse\u001b[39;00m, [\u001b[38;5;241m0\u001b[39m], groups, (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m))[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:  # pre-train the model\n",
    "    progress_bar = tqdm.trange(pretrain_n_epochs, desc=\"Epoch\")\n",
    "    for epoch in progress_bar:\n",
    "        kappa = min(epoch * pretrain_reg_strength, 0.5)\n",
    "        for i, (x, u) in enumerate(dl_pretrain):\n",
    "            # Forward pass\n",
    "            u, x = u.to(device), x.to(device)\n",
    "            output = model(x)\n",
    "            bce_loss = criterion(output.squeeze().float(), u.squeeze().float())\n",
    "            if bce_loss.item() < 0.05:\n",
    "                break\n",
    "            if kappa > 0:\n",
    "                regularization = parameter_gradient_interval_regularizer(\n",
    "                    model, x, u, \"binary_cross_entropy\", pretrain_epsilon, pretrain_model_epsilon\n",
    "                )\n",
    "            else:\n",
    "                regularization = torch.tensor(0.0)\n",
    "            loss = bce_loss + kappa * regularization\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                progress_bar.set_postfix(loss=loss.item(), bce_loss=bce_loss.item(), reg=regularization.item(), reg_strength=kappa)\n",
    "    # save the model\n",
    "    # with open(model_path, \"wb\") as file:\n",
    "    #     torch.save(model.state_dict(), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the robustness of the model pre-trained with the gradient interval regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robustly trained classifier on test set with epsilon=0.0005: [0.91, 0.93, 0.95]\n"
     ]
    }
   ],
   "source": [
    "_, dl_test = oct_mnist.get_dataloaders(1000, exclude_classes=[2], balanced=True)\n",
    "params_l, params_n, params_u = ct_utils.get_parameters(model)\n",
    "epsilon = 0.0005\n",
    "test_batch, test_labels = next(iter(dl_test))\n",
    "accs = agt.test_metrics.test_accuracy(\n",
    "    params_l, params_n, params_u, test_batch, test_labels, model, ct_utils.propagate_conv_layers, epsilon\n",
    ")\n",
    "accs = \", \".join([f\"{a:.2f}\" for a in accs])\n",
    "\n",
    "print(f\"Accuracy of robustly trained classifier on test set with epsilon={epsilon}: [{accs}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the model with AGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up fine-tuning parameters\n",
    "batchsize = 5000\n",
    "config = AGTConfig(\n",
    "    fragsize=1000,\n",
    "    learning_rate=0.3,\n",
    "    n_epochs=1,\n",
    "    k_private=10,\n",
    "    clip_gamma=2.0,\n",
    "    clip_method=\"clamp\",\n",
    "    dp_sgd_sigma=0.0,\n",
    "    forward_bound=\"interval\",\n",
    "    device=\"cuda:0\",\n",
    "    backward_bound=\"interval\",\n",
    "    loss=\"binary_cross_entropy\",\n",
    "    log_level=\"DEBUG\",\n",
    "    lr_decay=0.3,\n",
    "    lr_min=0.001,\n",
    "    early_stopping=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloaders, train dataloader is a mix of drusen and the \"healthy\" class\n",
    "dl_train, _ = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1], balanced=True)\n",
    "_, dl_test_drusen = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1, 3])\n",
    "_, dl_test_other = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[2])\n",
    "_, dl_test_all = oct_mnist.get_dataloaders(batchsize, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Pre-trained model accuracy ===========\n",
      "Class 2 (Drusen) : nominal = 0.38\n",
      "Classes 0, 1, 3  : nominal = 0.94\n",
      "All Classes      : nominal = 0.8\n"
     ]
    }
   ],
   "source": [
    "# evaluate the pre-trained model\n",
    "param_n, param_l, param_u = ct_utils.get_parameters(model)\n",
    "drusen_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_drusen)), model, ct_utils.propagate_conv_layers)\n",
    "other_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_other)), model, ct_utils.propagate_conv_layers)\n",
    "all_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers)\n",
    "\n",
    "print(\"=========== Pre-trained model accuracy ===========\")\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}\")\n",
    "print(f\"Classes 0, 1, 3  : nominal = {other_acc[1]:.2g}\")\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [INFO    ] [19:29:42] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [DEBUG   ] [19:29:42] \tPrivacy parameters: k_private=10, clip_gamma=2.0, dp_sgd_sigma=0.0\n",
      "[AGT] [DEBUG   ] [19:29:42] \tBounding methods: forward=interval, backward=interval\n",
      "[AGT] [INFO    ] [19:29:42] Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [DEBUG   ] [19:29:42] Initialising dataloader batchsize to 5000\n",
      "[AGT] [INFO    ] [19:29:42] Training batch 1: Network eval bounds=(0.38, 0.38, 0.38), W0 Bound=0.0 \n",
      "[AGT] [INFO    ] [19:29:43] Training batch 2: Network eval bounds=(1   , 1   , 1   ), W0 Bound=1.36 \n",
      "[AGT] [INFO    ] [19:29:44] Training batch 3: Network eval bounds=(0.23, 0.31, 0.5 ), W0 Bound=2.41 \n",
      "[AGT] [DEBUG   ] [19:29:45] Skipping batch 4 in epoch 1 (expected batchsize 5000, got 508)\n",
      "[AGT] [INFO    ] [19:29:45] Final network eval: Network eval bounds=(0.46, 0.9 , 1   ), W0 Bound=3.29 \n",
      "[AGT] [INFO    ] [19:29:45] =================== Finished Privacy Certified Training ===================\n"
     ]
    }
   ],
   "source": [
    "# fine-tune the model using abstract gradient training (keeping the convolutional layers fixed)\n",
    "param_l, param_n, param_u = agt.privacy_certified_training(\n",
    "    model, config, dl_train, dl_test_drusen, transform=ct_utils.propagate_conv_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Fine-tuned model accuracy + bounds ===========\n",
      "Class 2 (Drusen) : nominal = 0.9, certified bound = 0.46\n",
      "Classes 0, 1, 3  : nominal = 0.81, certified bound = 0.63\n",
      "All Classes      : nominal = 0.83, certified bound = 0.59\n"
     ]
    }
   ],
   "source": [
    "# evaluate the fine-tuned model\n",
    "drusen_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_drusen)), model, ct_utils.propagate_conv_layers)\n",
    "other_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_other)), model, ct_utils.propagate_conv_layers)\n",
    "all_acc = agt.test_metrics.test_accuracy(param_n, param_l, param_u, *next(iter(dl_test_all)), model, ct_utils.propagate_conv_layers)\n",
    "\n",
    "print(\"=========== Fine-tuned model accuracy + bounds ===========\")\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}, certified bound = {drusen_acc[0]:.2g}\")\n",
    "print(f\"Classes 0, 1, 3  : nominal = {other_acc[1]:.2g}, certified bound = {other_acc[0]:.2g}\")\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}, certified bound = {all_acc[0]:.2g}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
