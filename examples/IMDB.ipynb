{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/vol/bitbucket/mg2720/llm/huggingface'\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import abstract_gradient_training as agt\n",
    "import models.llm as llm\n",
    "import models.robust_regularizer\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training import certified_training_utils as ct_utils\n",
    "from models.pipeline import train_llm_with_guidance, test_llm_accuracy, write_results_to_file\n",
    "from datasets.imdb import get_loader_from_dataset, ImdbDataset\n",
    "from datasets.spurious_words import all_imdb_spur\n",
    "from metrics import llm_restart_avg_and_worst_group_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 88\n",
    "test_batch_size = 250\n",
    "criterion = torch.nn.BCELoss()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_tokenizer = llm.BertTokenizerWrapper(all_imdb_spur())\n",
    "imdb_train = ImdbDataset(is_train=True)\n",
    "imdb_test = ImdbDataset(is_train=False, grouped=True)\n",
    "dl_masks_train, dl_masks_test = get_loader_from_dataset(imdb_train, batch_size=batch_size), get_loader_from_dataset(imdb_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root_save_dir = \"saved_experiment_models/performance/imdb\"\n",
    "os.makedirs(model_root_save_dir, exist_ok=True)\n",
    "methods = [\"std\", \"r3\", \"smooth_r3\", \"rand_r4\", \"pgd_r4\"]\n",
    "save_dir_for_method = {method: os.path.join(model_root_save_dir, method) for method in methods}\n",
    "for method in methods:\n",
    "    os.makedirs(save_dir_for_method[method], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_method = \"std\"\n",
    "# hyperparameters\n",
    "num_epochs, lr, restarts, weight_decay, lmbda = 2, 0.0001, 1, 0, 0\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "train_acc, test_acc = 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + seed)\n",
    "    curr_model = llm.BertModelWrapper(1, device)\n",
    "\n",
    "    print(f\"========== Training model with method {std_method} restart {i} ==========\")\n",
    "    train_llm_with_guidance(curr_model, bert_tokenizer, dl_masks_train, num_epochs, lr, criterion, std_method, lmbda, device, weight_decay=weight_decay)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_test, device)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[std_method], f\"run_{i}.pt\"))\n",
    "empty_model = llm.BertModelWrapper(1, device)\n",
    "avg_acc, wg_acc, wg, *_ = llm_restart_avg_and_worst_group_acc(\n",
    "    dl_masks_test, save_dir_for_method[std_method], empty_model, bert_tokenizer, device, num_groups=2\n",
    ")\n",
    "write_results_to_file(\"experiment_results/imdb_bert.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg}, std_method)\n",
    "write_results_to_file(\"experiment_results/imdb_bert_params.yaml\",\n",
    "                        {\"k\": lmbda,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": False}, std_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(models.pipeline)\n",
    "reload(models.robust_regularizer)\n",
    "r3_method = \"r3\"\n",
    "# hyperparameters\n",
    "num_epochs, lr, restarts, weight_decay, lmbda, num_frags = 3, 0.0001, 1, 0, 5e+8, 5\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "train_acc, test_acc = 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + seed)\n",
    "    curr_model = llm.BertModelWrapper(1, device)\n",
    "\n",
    "    print(f\"========== Training model with method {r3_method} restart {i} ==========\")\n",
    "    train_llm_with_guidance(\n",
    "        curr_model, bert_tokenizer, dl_masks_train, num_epochs, lr, criterion, r3_method, lmbda, device, num_fragments=num_frags, weight_decay=weight_decay\n",
    "    )\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_test, device)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[r3_method], f\"run_{i}.pt\"))\n",
    "empty_model = llm.BertModelWrapper(1, device)\n",
    "avg_acc, wg_acc, wg, *_ = llm_restart_avg_and_worst_group_acc(\n",
    "    dl_masks_test, save_dir_for_method[r3_method], empty_model, bert_tokenizer, device, num_groups=2\n",
    ")\n",
    "write_results_to_file(\"experiment_results/imdb_bert.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg}, r3_method)\n",
    "write_results_to_file(\"experiment_results/imdb_bert_params.yaml\",\n",
    "                        {\"k\": lmbda,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": False}, r3_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth-R3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(models.pipeline)\n",
    "reload(models.robust_regularizer)\n",
    "smooth_r3_method = \"smooth_r3\"\n",
    "# hyperparameters\n",
    "num_epochs, lr, restarts, weight_decay, lmbda, num_frags, n_samples, alpha = 2, 0.00005, 1, 0, 1e+9, 11, 3, 0.1\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "train_acc, test_acc = 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + seed)\n",
    "    curr_model = llm.BertModelWrapper(1, device)\n",
    "\n",
    "    print(f\"========== Training model with method {smooth_r3_method} restart {i} ==========\")\n",
    "    train_llm_with_guidance(\n",
    "        curr_model, bert_tokenizer, dl_masks_train, num_epochs, lr, criterion, smooth_r3_method, lmbda, device,\n",
    "        num_fragments=num_frags, weight_decay=weight_decay, num_samples=n_samples, alpha=alpha\n",
    "    )\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_test, device)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[smooth_r3_method], f\"run_{i}.pt\"))\n",
    "empty_model = llm.BertModelWrapper(1, device)\n",
    "avg_acc, wg_acc, wg, *_ = llm_restart_avg_and_worst_group_acc(\n",
    "    dl_masks_test, save_dir_for_method[smooth_r3_method], empty_model, bert_tokenizer, device, num_groups=2\n",
    ")\n",
    "write_results_to_file(\"experiment_results/imdb_bert.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg}, smooth_r3_method)\n",
    "write_results_to_file(\"experiment_results/imdb_bert_params.yaml\",\n",
    "                        {\"k\": lmbda,\n",
    "                         \"alpha\": alpha,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"num_samples\": n_samples,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": False}, smooth_r3_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rand-R4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(models.pipeline)\n",
    "reload(models.robust_regularizer)\n",
    "rand_r4_method = \"rand_r4\"\n",
    "# hyperparameters\n",
    "num_epochs, lr, restarts, weight_decay, lmbda, num_frags, n_samples, alpha = 1, 0.00005, 1, 0, 5e+10, 11, 3, 0.75\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "train_acc, test_acc = 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + seed)\n",
    "    curr_model = llm.BertModelWrapper(1, device)\n",
    "\n",
    "    print(f\"========== Training model with method {rand_r4_method} restart {i} ==========\")\n",
    "    train_llm_with_guidance(\n",
    "        curr_model, bert_tokenizer, dl_masks_train, num_epochs, lr, criterion, rand_r4_method, lmbda, device,\n",
    "        num_fragments=num_frags, weight_decay=weight_decay, num_samples=n_samples, alpha=alpha\n",
    "    )\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_test, device)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[rand_r4_method], f\"run_{i}.pt\"))\n",
    "empty_model = llm.BertModelWrapper(1, device)\n",
    "avg_acc, wg_acc, wg, *_ = llm_restart_avg_and_worst_group_acc(\n",
    "    dl_masks_test, save_dir_for_method[rand_r4_method], empty_model, bert_tokenizer, device, num_groups=2\n",
    ")\n",
    "write_results_to_file(\"experiment_results/imdb_bert.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg}, rand_r4_method)\n",
    "write_results_to_file(\"experiment_results/imdb_bert_params.yaml\",\n",
    "                        {\"k\": lmbda,\n",
    "                         \"alpha\": alpha,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"num_samples\": n_samples,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": False}, rand_r4_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCG Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(models.pipeline)\n",
    "reload(models.robust_regularizer)\n",
    "pgd_r4_method = \"pgd_r4\"\n",
    "# hyperparameters\n",
    "num_epochs, lr, restarts, weight_decay, lmbda, num_frags, alpha = 1, 0.00005, 1, 0, 5e+10, 6, 0.3\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "train_acc, test_acc = 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + seed)\n",
    "    curr_model = llm.BertModelWrapper(1, device)\n",
    "\n",
    "    print(f\"========== Training model with method {pgd_r4_method} restart {i} ==========\")\n",
    "    train_llm_with_guidance(\n",
    "        curr_model, bert_tokenizer, dl_masks_train, num_epochs, lr, criterion, pgd_r4_method,\n",
    "        lmbda, device, num_fragments=num_frags, weight_decay=weight_decay\n",
    "    )\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_llm_accuracy(curr_model, bert_tokenizer, dl_masks_test, device)\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[pgd_r4_method], f\"run_{i}.pt\"))\n",
    "empty_model = llm.BertModelWrapper(1, device)\n",
    "avg_acc, wg_acc, wg, *_ = llm_restart_avg_and_worst_group_acc(\n",
    "    dl_masks_test, save_dir_for_method[pgd_r4_method], empty_model, bert_tokenizer, device, num_groups=2\n",
    ")\n",
    "write_results_to_file(\"experiment_results/imdb_bert.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 5),\n",
    "                       \"test_acc\": round(test_acc / restarts, 5),\n",
    "                       \"avg_group_acc\": round(avg_acc, 5),\n",
    "                       \"worst_group_acc\": round(wg_acc, 5),\n",
    "                       \"worst_group\": wg}, pgd_r4_method)\n",
    "write_results_to_file(\"experiment_results/imdb_bert_params.yaml\",\n",
    "                        {\"k\": lmbda,\n",
    "                         \"alpha\": alpha,\n",
    "                         \"weight_decay\": weight_decay,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"train_batch_size\": dl_masks_train.batch_size,\n",
    "                         \"test_batch_size\": dl_masks_test.batch_size,\n",
    "                         \"class_weights\": -1,\n",
    "                         \"multi_class\": False}, pgd_r4_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
