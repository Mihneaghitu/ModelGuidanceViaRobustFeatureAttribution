{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R4 on ISIC 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import certified_training_utils as ct_utils\n",
    "from models.R4_models import LesionNet\n",
    "from models.robust_regularizer import input_gradient_interval_regularizer\n",
    "from datasets import isic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "SEED = 0\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.manual_seed(SEED)\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isic_save_path = \"/vol/bitbucket/mg2720/isic/isic.pt\"\n",
    "# [isic_input_tensors, isic_labels] = torch.load(isic_save_path, mmap=True)\n",
    "DATA_ROOT = \"/vol/bitbucket/mg2720/isic/isic_train_new\"\n",
    "METADATA_FILE = \"/vol/bitbucket/mg2720/isic/isic_train_new/metadata.csv\"\n",
    "MASKS_ROOT = \"/vol/bitbucket/mg2720/isic/segmentation\"\n",
    "\n",
    "isic_train = isic.ISICDataset(DATA_ROOT, METADATA_FILE, is_train=True, masks_root=MASKS_ROOT)\n",
    "isic_test = isic.ISICDataset(DATA_ROOT, METADATA_FILE, is_train=False)\n",
    "img, lbl, mask = isic_train[1]\n",
    "print(img.shape, lbl, mask.shape)\n",
    "print(len(isic_train), len(isic_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.permute(1, 2, 0).squeeze().numpy())\n",
    "# plt.imshow(mask.squeeze().numpy())\n",
    "plt.colorbar()\n",
    "print(mask.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = isic.get_loader_from_dataset(isic_train, batch_size=batch_size, shuffle=False)\n",
    "dl_test = isic.get_loader_from_dataset(isic_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with(n_epochs, model, learning_rate, criterion, epsilon, mlx_method, k, has_conv):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # pre-train the model\n",
    "    progress_bar = tqdm.trange(n_epochs, desc=\"Epoch\", )\n",
    "    for _ in progress_bar:\n",
    "        for i, (x, u, m) in enumerate(dl_train):\n",
    "            # Forward pass\n",
    "            u, x, m = u.to(device).float(), x.to(device), m.to(device)\n",
    "\n",
    "            # output is [batch_size, 1], u is [bach_size] but BCELoss does not support different target and source sizes\n",
    "            output = model(x).squeeze()\n",
    "            # For std, we will waste some time doing the bounds, but at least it is consistent across methods\n",
    "            inp_grad_reg = input_gradient_interval_regularizer(\n",
    "                model, x, u, \"binary_cross_entropy\", epsilon, 0.0, regularizer_type=mlx_method, batch_masks=m, has_conv=has_conv\n",
    "            )\n",
    "            if mlx_method == \"std\":\n",
    "                assert inp_grad_reg == 0\n",
    "            loss = criterion(output, u) + k * inp_grad_reg\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                progress_bar.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_acc(model):\n",
    "    # evaluate the pre-trained model\n",
    "    # here, param_n, param_l, param_u are the parameters of the model and l and u are the same (we did not poison)\n",
    "    all_acc = 0\n",
    "    for test_batch, test_labels in dl_test:\n",
    "        # Just do a simple forward and compare the output to the labels\n",
    "        test_batch, test_labels = test_batch.to(device), test_labels.to(device)\n",
    "        output = model(test_batch).squeeze()\n",
    "        correct = ((output > 0.5) == (test_labels)).sum().item()\n",
    "        all_acc += correct\n",
    "    all_acc /= len(dl_test.dataset)\n",
    "    print(\"--- Pre-trained model accuracy ---\")\n",
    "    print(f\"Nominal = {all_acc:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient(model, batch_input, batch_labels, batch_mask, epsilon, elem_idx, has_conv):\n",
    "    channel_to_view = 1\n",
    "    batch_input, batch_labels = batch_input.to(device), batch_labels.to(device)\n",
    "    grad_bounds = input_gradient_interval_regularizer(\n",
    "        model, batch_input, batch_labels, \"binary_cross_entropy\", epsilon, 0.0, return_grads=True, regularizer_type=\"r4\", batch_masks=batch_mask, has_conv=has_conv\n",
    "    )\n",
    "    dx_l, dx_u = grad_bounds[1]\n",
    "    dx_n, _ = grad_bounds[0]\n",
    "    print(f\"input lower bound shape: {dx_l.shape}\")\n",
    "    print(f\"input upper bound shape: {dx_u.shape}\")\n",
    "    print(f\"input gradient shape: {dx_n.shape}\")\n",
    "    fig, ax = plt.subplots(3, 2, figsize=(14, 13))\n",
    "    lesion = batch_input[elem_idx].permute(1, 2, 0).cpu().numpy()\n",
    "    mask = batch_mask[elem_idx].cpu().numpy()\n",
    "    # choose only 1 channel gradient to view, because with 3 channels, the bounds do not represent rgb values\n",
    "    dx_l_view, dx_u_view, dx_n_view = dx_l[elem_idx][channel_to_view].squeeze(), dx_u[elem_idx][channel_to_view].squeeze(), dx_n[elem_idx][channel_to_view].squeeze()\n",
    "    ax[0][0].imshow(lesion)\n",
    "    ax[0][0].set_title(f\"Input at index {elem_idx}\")\n",
    "    im_mask = ax[0][1].imshow(mask, cmap='gray')\n",
    "    ax[0][1].set_title(f\"Mask at index {elem_idx}\")\n",
    "    fig.colorbar(im_mask, ax=ax[0][1])\n",
    "    im_dx_l = ax[1][0].imshow(dx_l_view.cpu().detach().numpy())\n",
    "    ax[1][0].set_title(f\"Lower bound of gradient at index {elem_idx}\")\n",
    "    fig.colorbar(im_dx_l, ax=ax[1][0])\n",
    "    im_dx_u = ax[1][1].imshow(dx_u_view.cpu().detach().numpy())\n",
    "    ax[1][1].set_title(f\"Upper bound of gradient at index {elem_idx}\")\n",
    "    fig.colorbar(im_dx_u, ax=ax[1][1])\n",
    "    im_dx_n = ax[2][0].imshow(dx_n_view.cpu().detach().numpy())\n",
    "    ax[2][0].set_title(f\"Gradient at index {elem_idx}\")\n",
    "    fig.colorbar(im_dx_n, ax=ax[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, dataset and optimizer\n",
    "num_epochs = 1\n",
    "lr = 0.01\n",
    "restarts = 3\n",
    "epsilon = 0.01\n",
    "# k is a coefficient for the regularization term\n",
    "k = 0.2\n",
    "model = LesionNet(3, 1)\n",
    "criterion = torch.nn.BCELoss()\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_visualize = torch.randint(0, batch_size, (1,)).item()\n",
    "print(idx_to_visualize)\n",
    "init_batch_train_with_masks = next(iter(dl_train))\n",
    "print(f\"Batch input shape: {init_batch_train_with_masks[0].shape}, batch mask shape: {init_batch_train_with_masks[2].shape}\")\n",
    "visualize_gradient(model, *init_batch_train_with_masks, epsilon, idx_to_visualize, has_conv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_INPUT_ROBUSTNESS_PARAM = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_delta_input_robustness(model):\n",
    "    # The model needs to be delta input robust only in the irrelevant features\n",
    "    num_robust = 0\n",
    "    for test_batch, test_labels, test_masks in dl_test:\n",
    "        test_batch, test_labels, test_masks = test_batch.to(device), test_labels.to(device), test_masks.to(device)\n",
    "        test_batch = test_batch.flatten(start_dim=1)\n",
    "        test_masks = test_masks.flatten(start_dim=1)\n",
    "        # The MLX method does not really matter, as we return the grads\n",
    "        grad_bounds = input_gradient_interval_regularizer(model, test_batch, test_labels, \"binary_cross_entropy\",\n",
    "            epsilon, 0.0, return_grads=True, batch_masks=test_masks, device=device)\n",
    "        d_l, d_u = grad_bounds[1]\n",
    "        d_l, d_u = d_l.flatten(start_dim=1), d_u.flatten(start_dim=1)\n",
    "        d_l, d_u = d_l * test_masks, d_u * test_masks\n",
    "        abs_diff = torch.abs(d_l - d_u)\n",
    "        # [batch_size, input_dim]\n",
    "        robust_grad_inputs = torch.where(abs_diff <= DELTA_INPUT_ROBUSTNESS_PARAM, 1, 0).bool()\n",
    "        # [batch_size]\n",
    "        is_robust_grad = robust_grad_inputs.all(dim=-1)\n",
    "        num_robust += is_robust_grad.sum().item()\n",
    "    num_robust /= len(dl_test.dataset)\n",
    "    print(\"--- Pre-trained model delta input robustness ---\")\n",
    "    print(f\"Delta Input Robustness = {num_robust:.2g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_method = \"std\"\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = LesionNet(3, 1).to(device)\n",
    "\n",
    "    print(f\"========== Training model with method {mlx_method} restart {i} ==========\")\n",
    "    train_model_with(num_epochs, curr_model, lr, criterion, epsilon, mlx_method, k, has_conv=True)\n",
    "    test_model_acc(curr_model)\n",
    "    # test_delta_input_robustness(curr_model)\n",
    "    if i == restarts - 1:\n",
    "        visualize_gradient(curr_model, *init_batch_train_with_masks, epsilon, idx_to_visualize, has_conv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RRR Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "rrr_method = \"r3\"\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = LesionNet(3, 1).to(device)\n",
    "\n",
    "    print(f\"========== Training model with method {rrr_method}, restart {i} ==========\")\n",
    "    train_model_with(num_epochs, curr_model, lr, criterion, epsilon, rrr_method, 1e-2, has_conv=True)\n",
    "    test_model_acc(curr_model)\n",
    "    test_delta_input_robustness(curr_model)\n",
    "    if i == restarts - 1:\n",
    "        visualize_gradient(curr_model, *init_batch_train_with_masks, epsilon, idx_to_visualize, has_conv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "r4_method = \"r4\"\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = LesionNet(3, 1).to(device)\n",
    "\n",
    "    print(f\"========== Training model with method R4, restart {i} ==========\")\n",
    "    train_model_with(num_epochs, curr_model, lr, criterion, epsilon, r4_method, k, has_conv=True)\n",
    "    test_model_acc(curr_model)\n",
    "    test_delta_input_robustness(curr_model)\n",
    "    if i == restarts - 1:\n",
    "        visualize_gradient(curr_model, *init_batch_train_with_masks, epsilon, idx_to_visualize, has_conv=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
