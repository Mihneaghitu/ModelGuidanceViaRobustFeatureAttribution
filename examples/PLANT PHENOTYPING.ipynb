{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R4 on Plant Phenotyping Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from models.R4_models import PlantNet\n",
    "from models.robust_regularizer import input_gradient_interval_regularizer\n",
    "from models.pipeline import (train_model_with_pgd_robust_input_grad, train_model_with_certified_input_grad,\n",
    "                             test_model_accuracy, test_delta_input_robustness, write_results_to_file,\n",
    "                             load_params_or_results_from_file, uniformize_magnitudes_schedule,\n",
    "                             train_model_with_smoothed_input_grad)\n",
    "from datasets import plant\n",
    "from metrics import worst_group_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "SEED = 0\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.manual_seed(SEED)\n",
    "batch_size = 50\n",
    "test_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_ROOT = \"/vol/bitbucket/mg2720/plant/rgb_dataset_splits\"\n",
    "DATA_ROOT = \"/vol/bitbucket/mg2720/plant/rgb_data\"\n",
    "MASKS_FILE = \"/vol/bitbucket/mg2720/plant/mask/preprocessed_masks.pyu\"\n",
    "\n",
    "plant_train_2 = plant.PlantDataset(SPLIT_ROOT, DATA_ROOT, MASKS_FILE, 2, True)\n",
    "plant_test_2 = plant.PlantDataset(SPLIT_ROOT, DATA_ROOT, MASKS_FILE, 2, False)\n",
    "print(len(plant_train_2), len(plant_test_2))\n",
    "num_neg, num_pos = (plant_train_2.data_labels == 0).sum(), (plant_train_2.data_labels == 1).sum()\n",
    "class_weights = [1.75 * num_pos / num_neg, 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = plant.get_dataloader(plant_train_2, batch_size)\n",
    "dl_test = plant.get_dataloader(plant_test_2, test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient(model, batch_input, batch_labels, batch_mask, epsilon, elem_idx, has_conv, curr_device):\n",
    "    channel_to_view = 1\n",
    "    batch_input, batch_labels, batch_mask = batch_input.to(curr_device), batch_labels.to(curr_device), batch_mask.to(curr_device)\n",
    "    model.to(curr_device)\n",
    "    grad_bounds = input_gradient_interval_regularizer(\n",
    "        model, batch_input, batch_labels, \"binary_cross_entropy\", epsilon, 0.0, return_grads=True, regularizer_type=\"r4\",\n",
    "        batch_masks=batch_mask, has_conv=has_conv, device=curr_device\n",
    "    )\n",
    "    dx_l, dx_u = grad_bounds[1]\n",
    "    dx_n, _ = grad_bounds[0]\n",
    "    print(f\"input lower bound shape: {dx_l.shape}\")\n",
    "    print(f\"input upper bound shape: {dx_u.shape}\")\n",
    "    print(f\"input gradient shape: {dx_n.shape}\")\n",
    "    fig, ax = plt.subplots(3, 2, figsize=(14, 13))\n",
    "    lesion = batch_input[elem_idx].permute(1, 2, 0).cpu().numpy()\n",
    "    mask = batch_mask[elem_idx].permute(1, 2, 0).cpu().numpy()\n",
    "    # choose only 1 channel gradient to view, because with 3 channels, the bounds do not represent rgb values\n",
    "    dx_l_view, dx_u_view, dx_n_view = dx_l[elem_idx][channel_to_view].squeeze(), dx_u[elem_idx][channel_to_view].squeeze(), dx_n[elem_idx][channel_to_view].squeeze()\n",
    "    ax[0][0].imshow(lesion)\n",
    "    ax[0][0].set_title(f\"Input at index {elem_idx}\")\n",
    "    im_mask = ax[0][1].imshow(mask, cmap='gray')\n",
    "    ax[0][1].set_title(f\"Mask at index {elem_idx}\")\n",
    "    fig.colorbar(im_mask, ax=ax[0][1])\n",
    "    im_dx_l = ax[1][0].imshow(dx_l_view.cpu().detach().numpy())\n",
    "    ax[1][0].set_title(f\"Lower bound of gradient at index {elem_idx}\")\n",
    "    fig.colorbar(im_dx_l, ax=ax[1][0])\n",
    "    im_dx_u = ax[1][1].imshow(dx_u_view.cpu().detach().numpy())\n",
    "    ax[1][1].set_title(f\"Upper bound of gradient at index {elem_idx}\")\n",
    "    cbu = fig.colorbar(im_dx_u, ax=ax[1][1])\n",
    "    cbu.ax.invert_yaxis()\n",
    "    im_dx_n = ax[2][0].imshow(dx_n_view.cpu().detach().numpy())\n",
    "    ax[2][0].set_title(f\"Gradient at index {elem_idx}\")\n",
    "    fig.colorbar(im_dx_n, ax=ax[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.01\n",
    "model = PlantNet(3, 1)\n",
    "criterion = torch.nn.BCELoss()\n",
    "gpu_ids = [i for i in range(torch.cuda.device_count())]\n",
    "print(gpu_ids)\n",
    "model = torch.nn.DataParallel(model, device_ids=gpu_ids)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_visualize = torch.randint(0, batch_size, (1,)).item()\n",
    "print(idx_to_visualize)\n",
    "init_batch_train_with_masks = next(iter(dl_train))\n",
    "print(f\"Batch input shape: {init_batch_train_with_masks[0].shape}, batch mask shape: {init_batch_train_with_masks[2].shape}\")\n",
    "# visualize_gradient(model, *init_batch_train_with_masks, epsilon, idx_to_visualize, True, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_INPUT_ROBUSTNESS_PARAM = 1\n",
    "model_root_save_dir = \"saved_experiment_models/performance/plant\"\n",
    "os.makedirs(model_root_save_dir, exist_ok=True)\n",
    "methods = [\"std\", \"r3\", \"r4\", \"ibp_ex\", \"ibp_ex+r3\", \"smooth_r3\", \"rand_r4\", \"pgd_r4\"]\n",
    "save_dir_for_method = {method: os.path.join(model_root_save_dir, method) for method in methods}\n",
    "for method in methods:\n",
    "    os.makedirs(save_dir_for_method[method], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_method = \"std\"\n",
    "# Hyperparameters\n",
    "num_epochs, lr, restarts, epsilon, k, weight_coeff = 7, 1e-3, 3, 0.1, 0.2, -1\n",
    "# k is a coefficient for the regularization term\n",
    "train_acc, test_acc, num_robust, min_robust_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 1e+8, 0, 0\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = PlantNet(3, 1)\n",
    "    print(f\"========== Training model with method {std_method} restart {i} ==========\")\n",
    "    train_model_with_certified_input_grad(dl_train, num_epochs, curr_model, lr, criterion, epsilon, std_method, k, device, True)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_test, device)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_test, curr_model, epsilon, DELTA_INPUT_ROBUSTNESS_PARAM,\n",
    "                                                 \"binary_cross_entropy\", device, has_conv=True)\n",
    "    num_robust += num_robust\n",
    "    min_robust_delta = min(min_robust_delta, min_delta)\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[std_method], f\"run_{i}.pt\"))\n",
    "write_results_to_file(\"experiment_results/plant.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 3),\n",
    "                       \"test_acc\": round(test_acc / restarts, 3),\n",
    "                       \"num_robust\": round(num_robust / restarts, 3),\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 3),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 3),\n",
    "                       \"min_robust_delta\": min_robust_delta}, std_method)\n",
    "write_results_to_file(\"experiment_results/plant_params.yaml\",\n",
    "                        {\"epsilon\": epsilon,\n",
    "                         \"k\": k,\n",
    "                         \"weight_coeff\": weight_coeff,\n",
    "                         \"num_epochs\": num_epochs,\n",
    "                         \"lr\": lr,\n",
    "                         \"restarts\": restarts,\n",
    "                         \"delta_threshold\": DELTA_INPUT_ROBUSTNESS_PARAM}, std_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RRR Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "rrr_method = \"r3\"\n",
    "# hyperparams\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k = 10, 3e-4, 3, 0.01, 0.004, 0.03\n",
    "alpha = 0.2\n",
    "train_acc, test_acc, num_robust, min_robust_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 1e+8, 0, 0\n",
    "soft_train_r3 = plant.make_soft_masks(dl_train, alpha)\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = PlantNet(3, 1)\n",
    "\n",
    "    print(f\"========== Training model with method {rrr_method}, restart {i} ==========\")\n",
    "    train_model_with_certified_input_grad(soft_train_r3, num_epochs, curr_model, lr, criterion, epsilon, rrr_method,\n",
    "        k, device, True, weight_reg_coeff=weight_coeff, class_weights=class_weights)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, soft_train_r3, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_test, device)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_test, curr_model, epsilon, DELTA_INPUT_ROBUSTNESS_PARAM,\n",
    "                                \"binary_cross_entropy\", device, has_conv=True)\n",
    "    num_robust += n_r\n",
    "    min_robust_delta = min(min_robust_delta, min_delta)\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[rrr_method], f\"run_{i}.pt\"))\n",
    "empty_model = PlantNet(3, 1).to(device)\n",
    "wg_acc, wg = worst_group_acc(empty_model, dl_test, device, 2, save_dir_for_method[rrr_method])\n",
    "write_results_to_file(\"experiment_results/plant.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 3),\n",
    "                       \"test_acc\": round(test_acc / restarts, 3),\n",
    "                       \"num_robust\": round(num_robust / restarts, 3),\n",
    "                       \"worst_group_acc\": round(wg_acc, 4),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 3),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 3),\n",
    "                       \"min_robust_delta\": min_robust_delta}, rrr_method)\n",
    "write_results_to_file(\"experiment_results/plant_params.yaml\",\n",
    "                      {\"epsilon\": epsilon,\n",
    "                       \"k\": k,\n",
    "                       \"weight_coeff\": weight_coeff,\n",
    "                       \"num_epochs\": num_epochs,\n",
    "                       \"lr\": lr,\n",
    "                       \"alpha_soft\": alpha,\n",
    "                       \"restarts\": restarts,\n",
    "                       \"delta_threshold\": DELTA_INPUT_ROBUSTNESS_PARAM}, rrr_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "r4_method = \"r4\"\n",
    "# hyperparams\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k, alpha = 14, 4e-4, 3, 0.01, -1, 0.03, 0.45\n",
    "train_acc, test_acc, num_robust, min_robust_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 1e+8, 0, 0\n",
    "soft_train = plant.make_soft_masks(dl_train, alpha)\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = PlantNet(3, 1)\n",
    "\n",
    "    print(f\"========== Training model with method {r4_method}, restart {i} ==========\")\n",
    "    train_model_with_certified_input_grad(soft_train, num_epochs, curr_model, lr, criterion, epsilon, r4_method,\n",
    "        k, device, True, weight_reg_coeff=weight_coeff, class_weights=class_weights)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, soft_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_test, device)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_test, curr_model, epsilon, DELTA_INPUT_ROBUSTNESS_PARAM,\n",
    "        \"binary_cross_entropy\", device, has_conv=True)\n",
    "    num_robust += n_r\n",
    "    min_robust_delta = min(min_robust_delta, min_delta)\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[r4_method], f\"run_{i}.pt\"))\n",
    "empty_model = PlantNet(3, 1).to(device)\n",
    "wg_acc, wg = worst_group_acc(empty_model, dl_test, device, 2, save_dir_for_method[r4_method])\n",
    "write_results_to_file(\"experiment_results/plant.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 3),\n",
    "                       \"test_acc\": round(test_acc / restarts, 3),\n",
    "                       \"num_robust\": round(num_robust / restarts, 3),\n",
    "                       \"worst_group_acc\": round(wg_acc, 4),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 3),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 3),\n",
    "                       \"min_robust_delta\": min_robust_delta}, r4_method)\n",
    "write_results_to_file(\"experiment_results/plant_params.yaml\",\n",
    "                      {\"epsilon\": epsilon,\n",
    "                       \"k\": k,\n",
    "                       \"weight_coeff\": weight_coeff,\n",
    "                       \"num_epochs\": num_epochs,\n",
    "                       \"lr\": lr,\n",
    "                       \"restarts\": restarts,\n",
    "                       \"delta_threshold\": DELTA_INPUT_ROBUSTNESS_PARAM,\n",
    "                       \"alpha_soft\": alpha}, r4_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBP_EX Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibp_ex_method = \"ibp_ex\"\n",
    "# hyperparams\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k, alpha = 10, 2e-4, 3, 0.01, 0.003, 0.4, 0.3\n",
    "train_acc, test_acc, num_robust, min_robust_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 1e+8, 0, 0\n",
    "soft_train_ibp_ex = plant.make_soft_masks(dl_train, alpha)\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = PlantNet(3, 1)\n",
    "\n",
    "    print(f\"========== Training model with method {ibp_ex_method}, restart {i} ==========\")\n",
    "    train_model_with_certified_input_grad(soft_train_ibp_ex, num_epochs, curr_model, lr, criterion, epsilon,\n",
    "        ibp_ex_method, k, device, True, weight_reg_coeff=weight_coeff, class_weights=class_weights)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_test, device)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_test, curr_model, epsilon, DELTA_INPUT_ROBUSTNESS_PARAM,\n",
    "                                \"binary_cross_entropy\", device, has_conv=True)\n",
    "    num_robust += n_r\n",
    "    min_robust_delta = min(min_robust_delta, min_delta)\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[ibp_ex_method], f\"run_{i}.pt\"))\n",
    "empty_model = PlantNet(3, 1).to(device)\n",
    "wg_acc, wg = worst_group_acc(empty_model, dl_test, device, 2, save_dir_for_method[ibp_ex_method])\n",
    "write_results_to_file(\"experiment_results/plant.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 3),\n",
    "                       \"test_acc\": round(test_acc / restarts, 3),\n",
    "                       \"num_robust\": round(num_robust / restarts, 3),\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 3),\n",
    "                       \"worst_group_acc\": round(wg_acc, 4),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 3),\n",
    "                       \"min_robust_delta\": min_robust_delta}, ibp_ex_method)\n",
    "write_results_to_file(\"experiment_results/plant_params.yaml\",\n",
    "                      {\"epsilon\": epsilon,\n",
    "                       \"k\": k,\n",
    "                       \"weight_coeff\": weight_coeff,\n",
    "                       \"num_epochs\": num_epochs,\n",
    "                       \"alpha_soft\": alpha,\n",
    "                       \"lr\": lr,\n",
    "                       \"restarts\": restarts,\n",
    "                       \"delta_threshold\": DELTA_INPUT_ROBUSTNESS_PARAM}, ibp_ex_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBP_EX+R3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibp_ex_and_r3_method = \"ibp_ex+r3\"\n",
    "# hyperparams\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k, alpha = 12, 5e-4, 3, 0.01, 0.002, 0.12, 0.35\n",
    "train_acc, test_acc, num_robust, min_robust_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 1e+8, 0, 0\n",
    "soft_train_ibp_ex_and_r3 = plant.make_soft_masks(dl_train, alpha)\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = PlantNet(3, 1)\n",
    "\n",
    "    print(f\"========== Training model with method {ibp_ex_and_r3_method}, restart {i} ==========\")\n",
    "    train_model_with_certified_input_grad(soft_train_ibp_ex_and_r3, num_epochs, curr_model, lr, criterion, epsilon,\n",
    "        ibp_ex_and_r3_method, k, device, True, weight_reg_coeff=weight_coeff, class_weights=class_weights)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, soft_train_ibp_ex_and_r3, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_test, device)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_test, curr_model, epsilon, DELTA_INPUT_ROBUSTNESS_PARAM,\n",
    "                                \"binary_cross_entropy\", device, has_conv=True)\n",
    "    num_robust += n_r\n",
    "    min_robust_delta = min(min_robust_delta, min_delta)\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[ibp_ex_and_r3_method], f\"run_{i}.pt\"))\n",
    "empty_model = PlantNet(3, 1).to(device)\n",
    "wg_acc, wg = worst_group_acc(empty_model, dl_test, device, 2, save_dir_for_method[ibp_ex_and_r3_method])\n",
    "write_results_to_file(\"experiment_results/plant.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 3),\n",
    "                       \"test_acc\": round(test_acc / restarts, 3),\n",
    "                       \"num_robust\": round(num_robust / restarts, 3),\n",
    "                       \"worst_group_acc\": round(wg_acc, 4),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 3),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 3),\n",
    "                       \"min_robust_delta\": min_robust_delta}, ibp_ex_and_r3_method)\n",
    "write_results_to_file(\"experiment_results/plant_params.yaml\",\n",
    "                      {\"epsilon\": epsilon,\n",
    "                       \"k\": k,\n",
    "                       \"weight_coeff\": weight_coeff,\n",
    "                       \"num_epochs\": num_epochs,\n",
    "                       \"lr\": lr,\n",
    "                       \"alpha_soft\": alpha,\n",
    "                       \"restarts\": restarts,\n",
    "                       \"delta_threshold\": DELTA_INPUT_ROBUSTNESS_PARAM}, ibp_ex_and_r3_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGD-R4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_r4 = \"pgd_r4\"\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k = 25, 5e-5, 3, 0.01, 1e-3, 0.75\n",
    "train_acc, test_acc, num_robust, min_robust_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 1e+8, 0, 0\n",
    "new_batch_size = 43 # len(dl_train) / 43 is an integer\n",
    "dl_train = plant.get_dataloader(plant_train_2, new_batch_size)\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = PlantNet(3, 1)\n",
    "\n",
    "    print(f\"========== Training model with method {pgd_r4} restart {i} ==========\")\n",
    "    train_model_with_pgd_robust_input_grad(dl_train, num_epochs, curr_model, lr, criterion, epsilon, pgd_r4,\n",
    "        k, device, weight_reg_coeff=weight_coeff, class_weights=class_weights, num_iterations=5)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_test, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_test, device)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_test, curr_model, epsilon,\n",
    "        DELTA_INPUT_ROBUSTNESS_PARAM,  \"binary_cross_entropy\", device, has_conv=True)\n",
    "    num_robust += n_r\n",
    "    min_robust_delta = min(min_robust_delta, min_delta)\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[pgd_r4], f\"run_{i}.pt\"))\n",
    "empty_model = PlantNet(3, 1)\n",
    "wg_acc, wg = worst_group_acc(curr_model, dl_test, device, 2, save_dir_for_method[pgd_r4])\n",
    "write_results_to_file(\"experiment_results/plant.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 3),\n",
    "                       \"test_acc\": round(test_acc / restarts, 3),\n",
    "                       \"num_robust\": round(num_robust / restarts, 3),\n",
    "                       \"worst_group_acc\": round(wg_acc, 3),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 3),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 3),\n",
    "                       \"min_robust_delta\": min_robust_delta}, pgd_r4)\n",
    "write_results_to_file(\"experiment_results/plant_params.yaml\",\n",
    "                      {\"epsilon\": epsilon,\n",
    "                       \"k\": k,\n",
    "                       \"weight_coeff\": weight_coeff,\n",
    "                       \"num_epochs\": num_epochs,\n",
    "                       \"lr\": lr,\n",
    "                       \"restarts\": restarts,\n",
    "                       \"delta_threshold\": DELTA_INPUT_ROBUSTNESS_PARAM}, pgd_r4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothed-R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_r3 = \"smooth_r3\"\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k = 20, 2e-5, 3, 0.01, 4e-4, 40\n",
    "train_acc, test_acc, num_robust, min_robust_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 1e+8, 0, 0\n",
    "new_batch_size = 43\n",
    "dl_train = plant.get_dataloader(plant_train_2, new_batch_size)\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = PlantNet(3, 1)\n",
    "\n",
    "    print(f\"========== Training model with method {smooth_r3} restart {i} ==========\")\n",
    "    train_model_with_smoothed_input_grad(dl_train, num_epochs, curr_model, lr, criterion, epsilon, smooth_r3, k,\n",
    "        device, weight_reg_coeff=weight_coeff, class_weights=class_weights)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_test, device)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_test, curr_model, epsilon,\n",
    "        DELTA_INPUT_ROBUSTNESS_PARAM, \"binary_cross_entropy\", device, has_conv=True)\n",
    "    num_robust += n_r\n",
    "    min_robust_delta = min(min_robust_delta, min_delta)\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[smooth_r3], f\"run_{i}.pt\"))\n",
    "empty_model = PlantNet(3, 1)\n",
    "wg_acc, wg = worst_group_acc(empty_model, dl_test, device, 2, save_dir_for_method[smooth_r3])\n",
    "write_results_to_file(\"experiment_results/plant.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 3),\n",
    "                       \"test_acc\": round(test_acc / restarts, 3),\n",
    "                       \"num_robust\": round(num_robust / restarts, 3),\n",
    "                       \"worst_group_acc\": round(wg_acc, 3),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 3),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 3),\n",
    "                       \"min_robust_delta\": min_robust_delta}, smooth_r3)\n",
    "write_results_to_file(\"experiment_results/plant_params.yaml\",\n",
    "                      {\"epsilon\": epsilon,\n",
    "                       \"k\": k,\n",
    "                       \"weight_coeff\": weight_coeff,\n",
    "                       \"num_epochs\": num_epochs,\n",
    "                       \"lr\": lr,\n",
    "                       \"restarts\": restarts,\n",
    "                       \"delta_threshold\": DELTA_INPUT_ROBUSTNESS_PARAM}, smooth_r3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rand-R4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_r4 = \"rand_r4\"\n",
    "# Train standard 3 times and test accuracy and delta input robustness for the masked region\n",
    "num_epochs, lr, restarts, epsilon, weight_coeff, k = 20, 2e-5, 3, 0.01, 4e-4, 35\n",
    "train_acc, test_acc, num_robust, min_robust_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 1e+8, 0, 0\n",
    "new_batch_size = 43\n",
    "dl_train = plant.get_dataloader(plant_train_2, new_batch_size)\n",
    "for i in range(restarts):\n",
    "    # Reinitialize the model\n",
    "    # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "    torch.manual_seed(i + SEED)\n",
    "    curr_model = PlantNet(3, 1)\n",
    "\n",
    "    print(f\"========== Training model with method {rand_r4} restart {i} ==========\")\n",
    "    train_model_with_smoothed_input_grad(dl_train, num_epochs, curr_model, lr, criterion, epsilon, rand_r4, k,\n",
    "        device, weight_reg_coeff=weight_coeff, class_weights=class_weights)\n",
    "    print(\"Testing model accuracy for the training set\")\n",
    "    train_acc += test_model_accuracy(curr_model, dl_train, device)\n",
    "    print(\"Testing model accuracy for the test set\")\n",
    "    test_acc += test_model_accuracy(curr_model, dl_test, device)\n",
    "    n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_test, curr_model, epsilon,\n",
    "        DELTA_INPUT_ROBUSTNESS_PARAM, \"binary_cross_entropy\", device, has_conv=True)\n",
    "    num_robust += n_r\n",
    "    min_robust_delta = min(min_robust_delta, min_delta)\n",
    "    min_lower_bound += m_l\n",
    "    max_upper_bound += m_u\n",
    "    torch.save(curr_model.state_dict(), os.path.join(save_dir_for_method[rand_r4], f\"run_{i}.pt\"))\n",
    "empty_model = PlantNet(3, 1)\n",
    "wg_acc, wg = worst_group_acc(empty_model, dl_test, device, 2, save_dir_for_method[rand_r4])\n",
    "write_results_to_file(\"experiment_results/plant.yaml\",\n",
    "                      {\"train_acc\": round(train_acc / restarts, 3),\n",
    "                       \"test_acc\": round(test_acc / restarts, 3),\n",
    "                       \"num_robust\": round(num_robust / restarts, 3),\n",
    "                       \"worst_group_acc\": round(wg_acc, 3),\n",
    "                       \"worst_group\": wg,\n",
    "                       \"min_lower_bound\": round(min_lower_bound / restarts, 3),\n",
    "                       \"max_upper_bound\": round(max_upper_bound / restarts, 3),\n",
    "                       \"min_robust_delta\": min_robust_delta}, rand_r4)\n",
    "write_results_to_file(\"experiment_results/plant_params.yaml\",\n",
    "                      {\"epsilon\": epsilon,\n",
    "                       \"k\": k,\n",
    "                       \"weight_coeff\": weight_coeff,\n",
    "                       \"num_epochs\": num_epochs,\n",
    "                       \"lr\": lr,\n",
    "                       \"restarts\": restarts,\n",
    "                       \"delta_threshold\": DELTA_INPUT_ROBUSTNESS_PARAM}, rand_r4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Complexity Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ratios = [0.8, 0.6, 0.4, 0.2]\n",
    "methods = [\"r3\", \"r4\", \"ibp_ex\", \"ibp_ex+r3\"]\n",
    "for method in methods:\n",
    "    # Load the params\n",
    "    params_dict = load_params_or_results_from_file(\"experiment_results/decoy_mnist_params.yaml\", method)\n",
    "    delta_threshold = params_dict[\"delta_threshold\"]\n",
    "    epsilon = params_dict[\"epsilon\"]\n",
    "    k = params_dict[\"k\"]\n",
    "    weight_coeff = params_dict[\"weight_coeff\"]\n",
    "    num_epochs = params_dict[\"num_epochs\"]\n",
    "    lr = params_dict[\"lr\"]\n",
    "    restarts = params_dict[\"restarts\"]\n",
    "    for mask_ratio in mask_ratios:\n",
    "        new_dl_train = plant.remove_masks(mask_ratio, dl_train)\n",
    "        if method == \"r4\":\n",
    "            new_dl_train = plant.make_soft_masks(new_dl_train, params_dict[\"alpha_soft\"])\n",
    "        train_acc, test_acc, num_robust, min_robust_delta, min_lower_bound, max_upper_bound = 0, 0, 0, 1e+8, 0, 0\n",
    "        for i in range(restarts):\n",
    "            # Reinitialize the model\n",
    "            # We could try to just reinitialize the weights, but we can throw away the previous model for now as we do not need it\n",
    "            torch.manual_seed(i + SEED)\n",
    "            curr_model = PlantNet(3, 1)\n",
    "\n",
    "            print(f\"========== Training model with method {method} restart {i} and mask ratio {mask_ratio} ==========\")\n",
    "            k_schedule = uniformize_magnitudes_schedule if method in [\"r3\", \"ibp_ex\", \"ibp_ex+r3\"] else None\n",
    "            train_model_with_certified_input_grad(new_dl_train, num_epochs, curr_model, lr, criterion, epsilon, method,\n",
    "                                                  k, device, True, weight_reg_coeff=weight_coeff, k_schedule=k_schedule)\n",
    "            train_acc += test_model_accuracy(curr_model, new_dl_train, device, suppress_log=True)\n",
    "            test_acc += test_model_accuracy(curr_model, dl_test, device, suppress_log=True)\n",
    "            n_r, min_delta, m_l, m_u = test_delta_input_robustness(dl_test, curr_model, epsilon, delta_threshold,\n",
    "                                                         \"binary_cross_entropy\", device, has_conv=True, suppress_log=True)\n",
    "            num_robust += n_r\n",
    "            min_robust_delta = min(min_robust_delta, min_delta)\n",
    "            min_lower_bound += m_l\n",
    "            max_upper_bound += m_u\n",
    "        write_results_to_file(f\"experiment_results/plant_sample_complexity.yaml\",\n",
    "                            {\"train_acc\": round(train_acc / restarts, 3),\n",
    "                             \"test_acc\": round(test_acc / restarts, 3),\n",
    "                             \"num_robust\": round(num_robust / restarts, 3),\n",
    "                             \"min_lower_bound_avg\": round(min_lower_bound / restarts, 3),\n",
    "                             \"max_upper_bound_avg\": round(max_upper_bound / restarts, 3),\n",
    "                             \"min_robust_delta\": min_robust_delta}, method + f\"_{int(mask_ratio * 100)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
